{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"faker-news","text":"<p>A Faker provider for generating fake news content using OpenAI-compatible LLM APIs.</p> <p>Generate realistic fake news headlines, article introductions, and full articles using any OpenAI-compatible LLM (OpenAI, Qwen, Azure, etc.) with intelligent caching and batch generation.</p>"},{"location":"#features","title":"Features","text":"<p>\ud83c\udfb2 Realistic Content Generation :   Generate fake news headlines, intros, and full articles that look authentic</p> <p>\ud83d\udcbe Smart SQLite Caching :   Efficiently cache and reuse generated content with usage tracking</p> <p>\ud83d\udd04 Batch Generation :   Minimize API calls by generating content in optimized batches</p> <p>\ud83d\udd0c Multi-Provider Support :   Works with OpenAI, Alibaba Qwen, Azure OpenAI, and any OpenAI-compatible API</p> <p>\ud83c\udfaf Lazy Loading :   Content is generated only when needed, not upfront</p> <p>\u267b\ufe0f Reusable Content :   Track usage and reuse content as needed with flexible consumption modes</p> <p>\ud83d\udd10 Secure Credentials :   API keys stored securely in your system's credential manager (Keychain/Credential Manager/Secret Service)</p>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from faker import Faker\nfrom faker_news import NewsProvider\n\nfake = Faker()\nfake.add_provider(NewsProvider(fake))\n\n# Generate a complete news article\nheadline = fake.news_headline()\nintro = fake.news_intro(headline=headline)\narticle = fake.news_article(headline=headline, words=500)\n\nprint(f\"{headline}\\n\\n{intro}\\n\\n{article}\")\n</code></pre> <p>Or use the CLI:</p> <pre><code># One-time setup\nfaker-news setup\n\n# Generate content\nfaker-news headline\nfaker-news article --words 800\nfaker-news preload --n 50  # Preload for faster access\n</code></pre>"},{"location":"#how-it-works","title":"How It Works","text":"<ol> <li>Headlines are pre-generated in bulk and cached locally in SQLite</li> <li>Intros and articles are generated on-demand in batches when needed</li> <li>Each item is marked as used after being fetched (configurable)</li> <li>When the pool runs low, new content is auto-generated in the background</li> <li>Content can be reused by resetting usage flags or fetching from the full pool</li> </ol> <p>This lazy batch approach minimizes LLM API calls while ensuring content is always available.</p>"},{"location":"#why-faker-news","title":"Why faker-news?","text":"<ul> <li>Testing: Generate realistic test data for news applications</li> <li>Demos: Create convincing placeholder content for prototypes</li> <li>Development: Populate databases with fake articles during development</li> <li>Privacy: No real news data needed - generate everything on-demand</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to generate fake news? Start with the Installation Guide or jump to the Quick Start.</p>"},{"location":"#license","title":"License","text":"<p>MIT License - see the repository for details.</p>"},{"location":"api-reference/","title":"API Reference","text":"<p>Complete API reference for faker-news.</p>"},{"location":"api-reference/#newsprovider","title":"NewsProvider","text":"<p>The main Faker provider for generating news content.</p>"},{"location":"api-reference/#constructor","title":"Constructor","text":"<pre><code>NewsProvider(\n    generator,\n    db_path=None,\n    min_headline_pool=30,\n    headline_batch=40,\n    intro_batch=20,\n    article_batch=10,\n    llm_config=None\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>generator</code> <code>Faker</code> Required Faker instance <code>db_path</code> <code>str | None</code> Platform-specific SQLite database file path <code>min_headline_pool</code> <code>int</code> <code>30</code> Minimum unused headlines before auto-refill <code>headline_batch</code> <code>int</code> <code>40</code> Headlines generated per batch <code>intro_batch</code> <code>int</code> <code>20</code> Intros generated per batch <code>article_batch</code> <code>int</code> <code>10</code> Articles generated per batch <code>llm_config</code> <code>LLMClientConfig | None</code> <code>None</code> LLM client configuration <p>Example:</p> <pre><code>from faker import Faker\nfrom faker_news import NewsProvider\n\nfake = Faker()\nprovider = NewsProvider(\n    fake,\n    db_path=\"/custom/cache.sqlite3\",\n    min_headline_pool=50,\n    headline_batch=60\n)\nfake.add_provider(provider)\n</code></pre>"},{"location":"api-reference/#news_headline","title":"news_headline()","text":"<p>Generate a fake news headline.</p> <pre><code>news_headline(consume=True, allow_used=False) -&gt; str\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>consume</code> <code>bool</code> <code>True</code> Mark headline as used after fetching <code>allow_used</code> <code>bool</code> <code>False</code> Fetch from all items (used or unused) <p>Returns: <code>str</code> - A fake news headline</p> <p>Example:</p> <pre><code># Generate and mark as used\nheadline = fake.news_headline()\n\n# Generate without consuming\nheadline = fake.news_headline(consume=False)\n\n# Fetch from all items\nheadline = fake.news_headline(allow_used=True)\n</code></pre>"},{"location":"api-reference/#news_intro","title":"news_intro()","text":"<p>Generate a fake news article introduction.</p> <pre><code>news_intro(headline=None, consume=True, allow_used=False) -&gt; str\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>headline</code> <code>str | None</code> <code>None</code> Specific headline to use (auto-generated if None) <code>consume</code> <code>bool</code> <code>True</code> Mark intro as used after fetching <code>allow_used</code> <code>bool</code> <code>False</code> Fetch from all items (used or unused) <p>Returns: <code>str</code> - A fake news intro</p> <p>Example:</p> <pre><code># Generate intro with auto-generated headline\nintro = fake.news_intro()\n\n# Generate intro for specific headline\nintro = fake.news_intro(headline=\"Breaking: Major Discovery Announced\")\n\n# Generate without consuming\nintro = fake.news_intro(consume=False)\n</code></pre>"},{"location":"api-reference/#news_article","title":"news_article()","text":"<p>Generate a complete fake news article with markdown formatting.</p> <pre><code>news_article(headline=None, words=500, consume=True, allow_used=False) -&gt; str\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>headline</code> <code>str | None</code> <code>None</code> Specific headline to use (auto-generated if None) <code>words</code> <code>int</code> <code>500</code> Target article length in words <code>consume</code> <code>bool</code> <code>True</code> Mark article as used after fetching <code>allow_used</code> <code>bool</code> <code>False</code> Fetch from all items (used or unused) <p>Returns: <code>str</code> - A fake news article with markdown formatting</p> <p>Example:</p> <pre><code># Generate ~500 word article\narticle = fake.news_article()\n\n# Generate longer article\narticle = fake.news_article(words=1000)\n\n# Generate for specific headline\narticle = fake.news_article(headline=\"Scientists Discover New Species\")\n\n# Generate without consuming\narticle = fake.news_article(consume=False)\n</code></pre>"},{"location":"api-reference/#news_preload_headlines","title":"news_preload_headlines()","text":"<p>Preload headlines in bulk for better performance.</p> <pre><code>news_preload_headlines(count) -&gt; None\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>count</code> <code>int</code> Required Number of headlines to generate <p>Example:</p> <pre><code># Preload 100 headlines\nfake.news_preload_headlines(100)\n\n# Now fetching is instant\nheadlines = [fake.news_headline() for _ in range(50)]\n</code></pre>"},{"location":"api-reference/#news_stats","title":"news_stats()","text":"<p>Get cache statistics.</p> <pre><code>news_stats() -&gt; dict\n</code></pre> <p>Returns: <code>dict</code> with the following keys:</p> Key Type Description <code>total</code> <code>int</code> Total number of items in cache <code>with_intro</code> <code>int</code> Items that have intros <code>with_article</code> <code>int</code> Items that have articles <code>unused_headlines</code> <code>int</code> Unused headlines available <code>unused_intros</code> <code>int</code> Unused intros available <code>unused_articles</code> <code>int</code> Unused articles available <p>Example:</p> <pre><code>stats = fake.news_stats()\nprint(f\"Total: {stats['total']}\")\nprint(f\"Unused headlines: {stats['unused_headlines']}\")\n</code></pre>"},{"location":"api-reference/#news_reset","title":"news_reset()","text":"<p>Reset usage flags or clear the cache.</p> <pre><code>news_reset(mode) -&gt; None\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>mode</code> <code>str</code> Required Reset mode: <code>\"reuse\"</code> or <code>\"clear\"</code> <p>Modes:</p> <ul> <li><code>\"reuse\"</code>: Mark all items as unused (keep content)</li> <li><code>\"clear\"</code>: Delete all cached content</li> </ul> <p>Example:</p> <pre><code># Mark all items as unused\nfake.news_reset(\"reuse\")\n\n# Clear all cached content\nfake.news_reset(\"clear\")\n</code></pre>"},{"location":"api-reference/#llmclientconfig","title":"LLMClientConfig","text":"<p>Configuration for the LLM client.</p>"},{"location":"api-reference/#constructor_1","title":"Constructor","text":"<pre><code>LLMClientConfig(\n    api_key=None,\n    base_url=None,\n    model_headlines=None,\n    model_writing=None\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>api_key</code> <code>str | None</code> Auto-detected LLM API key <code>base_url</code> <code>str | None</code> Auto-detected API endpoint URL <code>model_headlines</code> <code>str | None</code> Provider-specific Model for headline generation <code>model_writing</code> <code>str | None</code> Provider-specific Model for intro/article generation <p>Auto-Detection:</p> <p>If parameters are not specified, they are auto-detected from:</p> <ol> <li>System keyring (for <code>api_key</code>)</li> <li>Environment variables (<code>OPENAI_API_KEY</code>, <code>DASHSCOPE_API_KEY</code>, etc.)</li> <li>Default values based on detected provider</li> </ol> <p>Example:</p> <pre><code>from faker_news import LLMClientConfig\n\n# Explicit configuration\nllm_config = LLMClientConfig(\n    api_key=\"sk-...\",\n    base_url=\"https://api.openai.com/v1\",\n    model_headlines=\"gpt-4o-mini\",\n    model_writing=\"gpt-4o\"\n)\n\n# Auto-detect from environment\nllm_config = LLMClientConfig()\n</code></pre>"},{"location":"api-reference/#llmclient","title":"LLMClient","text":"<p>Low-level client for LLM API calls (not typically used directly).</p>"},{"location":"api-reference/#constructor_2","title":"Constructor","text":"<pre><code>LLMClient(config=None)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>config</code> <code>LLMClientConfig | None</code> Auto-detected LLM client configuration"},{"location":"api-reference/#generate_headlines","title":"generate_headlines()","text":"<p>Generate multiple headlines in one batch.</p> <pre><code>generate_headlines(count) -&gt; list[str]\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>count</code> <code>int</code> Required Number of headlines to generate <p>Returns: <code>list[str]</code> - List of generated headlines</p> <p>Example:</p> <pre><code>from faker_news import LLMClient\n\nclient = LLMClient()\nheadlines = client.generate_headlines(10)\n</code></pre>"},{"location":"api-reference/#generate_intros","title":"generate_intros()","text":"<p>Generate multiple intros for given headlines.</p> <pre><code>generate_intros(headlines) -&gt; list[str]\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>headlines</code> <code>list[str]</code> Required Headlines to generate intros for <p>Returns: <code>list[str]</code> - List of generated intros</p> <p>Example:</p> <pre><code>headlines = [\"Breaking News\", \"Tech Update\"]\nintros = client.generate_intros(headlines)\n</code></pre>"},{"location":"api-reference/#generate_articles","title":"generate_articles()","text":"<p>Generate multiple articles for given headlines.</p> <pre><code>generate_articles(headlines, words=500) -&gt; list[str]\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>headlines</code> <code>list[str]</code> Required Headlines to generate articles for <code>words</code> <code>int</code> <code>500</code> Target article length <p>Returns: <code>list[str]</code> - List of generated articles</p> <p>Example:</p> <pre><code>headlines = [\"Breaking News\"]\narticles = client.generate_articles(headlines, words=800)\n</code></pre>"},{"location":"api-reference/#newsstore","title":"NewsStore","text":"<p>Low-level SQLite storage layer (not typically used directly).</p>"},{"location":"api-reference/#constructor_3","title":"Constructor","text":"<pre><code>NewsStore(db_path=None)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>db_path</code> <code>str | None</code> Platform-specific SQLite database file path"},{"location":"api-reference/#add_headlines","title":"add_headlines()","text":"<p>Store headlines in the database.</p> <pre><code>add_headlines(headlines) -&gt; None\n</code></pre>"},{"location":"api-reference/#fetch_headline","title":"fetch_headline()","text":"<p>Fetch a random headline.</p> <pre><code>fetch_headline(mark_used=True, allow_used=False) -&gt; str | None\n</code></pre>"},{"location":"api-reference/#fetch_intro","title":"fetch_intro()","text":"<p>Fetch a random intro.</p> <pre><code>fetch_intro(headline=None, mark_used=True, allow_used=False) -&gt; str | None\n</code></pre>"},{"location":"api-reference/#fetch_article","title":"fetch_article()","text":"<p>Fetch a random article.</p> <pre><code>fetch_article(headline=None, mark_used=True, allow_used=False) -&gt; str | None\n</code></pre>"},{"location":"api-reference/#get_stats","title":"get_stats()","text":"<p>Get cache statistics.</p> <pre><code>get_stats() -&gt; dict\n</code></pre>"},{"location":"api-reference/#reset_usage","title":"reset_usage()","text":"<p>Reset all usage flags.</p> <pre><code>reset_usage() -&gt; None\n</code></pre>"},{"location":"api-reference/#clear_all","title":"clear_all()","text":"<p>Delete all cached items.</p> <pre><code>clear_all() -&gt; None\n</code></pre>"},{"location":"api-reference/#exceptions","title":"Exceptions","text":""},{"location":"api-reference/#apierror","title":"APIError","text":"<p>Raised when LLM API calls fail.</p> <pre><code>from faker_news.client import APIError\n\ntry:\n    headline = fake.news_headline()\nexcept APIError as e:\n    print(f\"API error: {e}\")\n</code></pre>"},{"location":"api-reference/#configurationerror","title":"ConfigurationError","text":"<p>Raised when configuration is invalid or missing.</p> <pre><code>from faker_news.client import ConfigurationError\n\ntry:\n    provider = NewsProvider(fake)\nexcept ConfigurationError as e:\n    print(f\"Configuration error: {e}\")\n</code></pre>"},{"location":"api-reference/#type-hints","title":"Type Hints","text":"<p>faker-news includes type hints for all public APIs:</p> <pre><code>from typing import Optional\nfrom faker import Faker\nfrom faker_news import NewsProvider, LLMClientConfig\n\ndef setup_provider(\n    fake: Faker,\n    api_key: Optional[str] = None\n) -&gt; NewsProvider:\n    \"\"\"Setup news provider with optional API key.\"\"\"\n    llm_config = LLMClientConfig(api_key=api_key) if api_key else None\n    return NewsProvider(fake, llm_config=llm_config)\n</code></pre>"},{"location":"api-reference/#next-steps","title":"Next Steps","text":"<ul> <li>Library Usage - Detailed usage examples</li> <li>Architecture - Understanding the internal architecture</li> <li>Contributing - Contributing to faker-news</li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>Understanding the internal architecture of faker-news.</p>"},{"location":"architecture/#overview","title":"Overview","text":"<p>faker-news uses a three-layer architecture:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      NewsProvider (Faker API)       \u2502  \u2190 User-facing API\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   LLMClient (API Communication)     \u2502  \u2190 LLM integration\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   NewsStore (SQLite Storage)        \u2502  \u2190 Data persistence\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Each layer has a specific responsibility and can be used independently.</p>"},{"location":"architecture/#layer-1-newsprovider","title":"Layer 1: NewsProvider","text":"<p>Location: <code>src/faker_news/provider.py</code></p> <p>Responsibility: Faker provider interface and high-level content management</p>"},{"location":"architecture/#key-features","title":"Key Features","text":"<ul> <li>Implements Faker provider interface</li> <li>Manages minimum pool thresholds</li> <li>Auto-refills pool when it runs low</li> <li>Lazy batch generation strategy</li> <li>User-facing API methods</li> </ul>"},{"location":"architecture/#how-it-works","title":"How It Works","text":"<pre><code>class NewsProvider(BaseProvider):\n    def news_headline(self, consume=True, allow_used=False):\n        # 1. Check pool size\n        stats = self.store.get_stats()\n        if stats['unused_headlines'] &lt; self.min_headline_pool:\n            # 2. Auto-refill if needed\n            headlines = self.client.generate_headlines(self.headline_batch)\n            self.store.add_headlines(headlines)\n\n        # 3. Fetch from cache\n        return self.store.fetch_headline(mark_used=consume, allow_used=allow_used)\n</code></pre>"},{"location":"architecture/#design-pattern-lazy-loading","title":"Design Pattern: Lazy Loading","text":"<p>Content is generated only when needed:</p> <ol> <li>Headlines: Pre-generated in bulk when pool drops below threshold</li> <li>Intros: Generated in batches only when first requested</li> <li>Articles: Generated in batches only when first requested</li> </ol> <p>This minimizes API calls while ensuring content availability.</p>"},{"location":"architecture/#layer-2-llmclient","title":"Layer 2: LLMClient","text":"<p>Location: <code>src/faker_news/client.py</code></p> <p>Responsibility: LLM API communication and content generation</p>"},{"location":"architecture/#key-features_1","title":"Key Features","text":"<ul> <li>OpenAI-compatible API client</li> <li>Batch generation for efficiency</li> <li>JSON parsing with fallback logic</li> <li>Auto-detects provider (OpenAI vs DashScope)</li> <li>Retry logic with exponential backoff</li> </ul>"},{"location":"architecture/#how-it-works_1","title":"How It Works","text":"<pre><code>class LLMClient:\n    def generate_headlines(self, count):\n        # 1. Build prompt\n        prompt = f\"Generate {count} fake news headlines...\"\n\n        # 2. Call LLM API\n        response = self.client.chat.completions.create(\n            model=self.config.model_headlines,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0.9\n        )\n\n        # 3. Parse JSON response\n        content = response.choices[0].message.content\n        headlines = self._parse_json(content)\n\n        return headlines\n</code></pre>"},{"location":"architecture/#batch-generation","title":"Batch Generation","text":"<p>All generation methods batch requests:</p> Method Batch Size API Calls <code>generate_headlines(40)</code> 40 1 <code>generate_intros(20)</code> 20 1 <code>generate_articles(10)</code> 10 1 <p>Each batch is sent in a single API call and receives all responses at once.</p>"},{"location":"architecture/#error-handling","title":"Error Handling","text":"<pre><code>def gen_json(self, prompt, model):\n    for attempt in range(3):  # Retry up to 3 times\n        try:\n            response = self.client.chat.completions.create(...)\n            return self._parse_json(response.choices[0].message.content)\n        except Exception as e:\n            if attempt == 2:\n                raise\n            time.sleep(0.8 * (attempt + 1))  # Exponential backoff\n</code></pre>"},{"location":"architecture/#connection-efficiency","title":"Connection Efficiency","text":"<ul> <li>OpenAI client instantiated once per <code>LLMClient</code></li> <li>httpx connection pooling with keep-alive</li> <li>Persistent connections reused across requests</li> </ul>"},{"location":"architecture/#layer-3-newsstore","title":"Layer 3: NewsStore","text":"<p>Location: <code>src/faker_news/store.py</code></p> <p>Responsibility: SQLite database management and content storage</p>"},{"location":"architecture/#database-schema","title":"Database Schema","text":"<pre><code>CREATE TABLE items (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    headline TEXT NOT NULL UNIQUE,\n    intro TEXT,\n    article TEXT,\n    used_headline BOOLEAN DEFAULT 0,\n    used_intro BOOLEAN DEFAULT 0,\n    used_article BOOLEAN DEFAULT 0\n);\n</code></pre>"},{"location":"architecture/#key-features_2","title":"Key Features","text":"<ul> <li>Three separate usage flags per item</li> <li>Random selection with <code>ORDER BY RANDOM()</code></li> <li>Atomic updates with COALESCE</li> <li>Efficient batch inserts</li> <li>Platform-specific cache directory</li> </ul>"},{"location":"architecture/#how-it-works_2","title":"How It Works","text":"<pre><code>class NewsStore:\n    def fetch_headline(self, mark_used=True, allow_used=False):\n        # Build query based on parameters\n        if allow_used:\n            query = \"SELECT headline FROM items ORDER BY RANDOM() LIMIT 1\"\n        else:\n            query = \"\"\"\n                SELECT headline FROM items\n                WHERE used_headline = 0\n                ORDER BY RANDOM() LIMIT 1\n            \"\"\"\n\n        # Fetch item\n        row = self.conn.execute(query).fetchone()\n        if not row:\n            return None\n\n        # Mark as used if requested\n        if mark_used:\n            self.conn.execute(\n                \"UPDATE items SET used_headline = 1 WHERE headline = ?\",\n                (row[0],)\n            )\n            self.conn.commit()\n\n        return row[0]\n</code></pre>"},{"location":"architecture/#usage-tracking","title":"Usage Tracking","text":"<p>Three independent usage flags:</p> <ul> <li><code>used_headline</code>: Headline has been consumed</li> <li><code>used_intro</code>: Intro has been consumed</li> <li><code>used_article</code>: Article has been consumed</li> </ul> <p>This allows: - Fetching the same headline multiple times with different intros - Reusing headlines while consuming intros/articles - Flexible content reuse strategies</p>"},{"location":"architecture/#atomic-updates","title":"Atomic Updates","text":"<pre><code>def add_intro(self, headline, intro):\n    # Only update if intro is NULL (don't overwrite existing)\n    self.conn.execute(\"\"\"\n        UPDATE items\n        SET intro = COALESCE(intro, ?)\n        WHERE headline = ?\n    \"\"\", (intro, headline))\n</code></pre>"},{"location":"architecture/#data-flow","title":"Data Flow","text":""},{"location":"architecture/#generating-a-complete-article","title":"Generating a Complete Article","text":"<pre><code>User calls: fake.news_article(headline=\"Breaking News\")\n    \u2502\n    \u251c\u2500\u2500&gt; NewsProvider.news_article()\n    \u2502       \u2502\n    \u2502       \u251c\u2500\u2500&gt; Check if article exists in cache\n    \u2502       \u2502    \u251c\u2500[YES]\u2500&gt; NewsStore.fetch_article()\n    \u2502       \u2502    \u2502              \u2514\u2500\u2500&gt; Return cached article\n    \u2502       \u2502    \u2502\n    \u2502       \u2502    \u2514\u2500[NO]\u2500\u2500&gt; Need to generate\n    \u2502       \u2502                \u2502\n    \u2502       \u2502                \u251c\u2500\u2500&gt; LLMClient.generate_articles([headline])\n    \u2502       \u2502                \u2502       \u2502\n    \u2502       \u2502                \u2502       \u251c\u2500\u2500&gt; Build prompt\n    \u2502       \u2502                \u2502       \u251c\u2500\u2500&gt; Call OpenAI API\n    \u2502       \u2502                \u2502       \u251c\u2500\u2500&gt; Parse JSON response\n    \u2502       \u2502                \u2502       \u2514\u2500\u2500&gt; Return [article]\n    \u2502       \u2502                \u2502\n    \u2502       \u2502                \u251c\u2500\u2500&gt; NewsStore.add_article(headline, article)\n    \u2502       \u2502                \u2514\u2500\u2500&gt; NewsStore.fetch_article(headline)\n    \u2502       \u2502\n    \u2502       \u2514\u2500\u2500&gt; Return article\n    \u2502\n    \u2514\u2500\u2500&gt; Article returned to user\n</code></pre>"},{"location":"architecture/#auto-refill-process","title":"Auto-Refill Process","text":"<pre><code>User calls: fake.news_headline()\n    \u2502\n    \u251c\u2500\u2500&gt; NewsProvider.news_headline()\n    \u2502       \u2502\n    \u2502       \u251c\u2500\u2500&gt; NewsStore.get_stats()\n    \u2502       \u2502       \u2514\u2500\u2500&gt; { unused_headlines: 25 }\n    \u2502       \u2502\n    \u2502       \u251c\u2500\u2500&gt; Check: 25 &lt; 30 (min_headline_pool)?\n    \u2502       \u2502    \u2514\u2500[YES]\u2500&gt; Auto-refill needed\n    \u2502       \u2502                \u2502\n    \u2502       \u2502                \u251c\u2500\u2500&gt; LLMClient.generate_headlines(40)\n    \u2502       \u2502                \u2502       \u2514\u2500\u2500&gt; [... 40 headlines ...]\n    \u2502       \u2502                \u2502\n    \u2502       \u2502                \u2514\u2500\u2500&gt; NewsStore.add_headlines(headlines)\n    \u2502       \u2502\n    \u2502       \u2514\u2500\u2500&gt; NewsStore.fetch_headline()\n    \u2502               \u2514\u2500\u2500&gt; Return headline\n    \u2502\n    \u2514\u2500\u2500&gt; Headline returned to user\n</code></pre>"},{"location":"architecture/#performance-optimizations","title":"Performance Optimizations","text":""},{"location":"architecture/#1-lazy-batch-generation","title":"1. Lazy Batch Generation","text":"<p>Instead of generating all content upfront:</p> <ul> <li>Headlines pre-generated in bulk</li> <li>Intros/articles generated only when needed</li> <li>Batched to minimize API calls</li> </ul>"},{"location":"architecture/#2-sqlite-caching","title":"2. SQLite Caching","text":"<p>All content cached locally:</p> <ul> <li>No repeated API calls for same content</li> <li>Random selection for variety</li> <li>Usage tracking to avoid repetition</li> </ul>"},{"location":"architecture/#3-connection-pooling","title":"3. Connection Pooling","text":"<p>HTTP connections reused:</p> <ul> <li>Single OpenAI client instance</li> <li>httpx connection pooling</li> <li>Keep-alive for persistent connections</li> </ul>"},{"location":"architecture/#4-batch-api-calls","title":"4. Batch API Calls","text":"<p>All generation happens in batches:</p> <pre><code># Inefficient: 40 API calls\nfor i in range(40):\n    headline = generate_one_headline()\n\n# Efficient: 1 API call\nheadlines = generate_headlines(40)\n</code></pre>"},{"location":"architecture/#5-smart-populate-mode","title":"5. Smart Populate Mode","text":"<p>Populating efficiently:</p> <pre><code>def populate(n):\n    # 1. Get unused headlines missing content\n    incomplete = fetch_headlines_needing_content(limit=n)\n\n    # 2. Use existing complete unused headlines\n    if len(incomplete) &lt; n:\n        complete = fetch_complete_unused_headlines(limit=n - len(incomplete))\n        incomplete.extend(complete)\n\n    # 3. Generate new only if still needed\n    if len(incomplete) &lt; n:\n        new_count = n - len(incomplete)\n        generate_new_headlines_with_content(new_count)\n</code></pre>"},{"location":"architecture/#code-organization","title":"Code Organization","text":"<pre><code>src/faker_news/\n\u251c\u2500\u2500 __init__.py       # Public API exports\n\u251c\u2500\u2500 client.py         # LLMClient and LLMClientConfig\n\u251c\u2500\u2500 store.py          # NewsStore (SQLite layer)\n\u251c\u2500\u2500 provider.py       # NewsProvider (Faker provider)\n\u251c\u2500\u2500 cli.py            # Command-line interface\n\u2514\u2500\u2500 setup.py          # Interactive setup script\n</code></pre>"},{"location":"architecture/#module-dependencies","title":"Module Dependencies","text":"<pre><code>cli.py \u2500\u2500\u2500\u2500\u2500\u2500\u2510\n             \u251c\u2500\u2500&gt; provider.py \u2500\u2500&gt; client.py\nsetup.py \u2500\u2500\u2500\u2500\u2524                       \u2502\n             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt; store.py\n</code></pre> <ul> <li><code>cli.py</code> and <code>setup.py</code> depend on all modules</li> <li><code>provider.py</code> orchestrates <code>client.py</code> and <code>store.py</code></li> <li><code>client.py</code> and <code>store.py</code> are independent</li> </ul>"},{"location":"architecture/#design-decisions","title":"Design Decisions","text":""},{"location":"architecture/#why-sqlite","title":"Why SQLite?","text":"<ul> <li>\u2705 Zero configuration required</li> <li>\u2705 Platform-independent</li> <li>\u2705 ACID transactions</li> <li>\u2705 Efficient random sampling</li> <li>\u2705 No external dependencies</li> <li>\u2705 Easy backup/migration</li> </ul>"},{"location":"architecture/#why-three-usage-flags","title":"Why Three Usage Flags?","text":"<p>Allows flexible content reuse:</p> <pre><code># Use same headline with different intros\nheadline = fake.news_headline(consume=False)\nintro1 = fake.news_intro(headline=headline)  # Consumes intro\nintro2 = fake.news_intro(headline=headline)  # Different intro\n\n# Headline still available (wasn't consumed)\nsame_headline = fake.news_headline()  # Might get same headline\n</code></pre>"},{"location":"architecture/#why-lazy-loading","title":"Why Lazy Loading?","text":"<p>Minimizes upfront cost:</p> <ul> <li>Don't generate articles if user only needs headlines</li> <li>Don't generate intros if user only needs articles</li> <li>Generate only what's actually used</li> </ul>"},{"location":"architecture/#why-batch-generation","title":"Why Batch Generation?","text":"<p>Efficiency:</p> <ul> <li>1 API call for 40 headlines vs. 40 API calls</li> <li>Lower latency (parallel processing by LLM)</li> <li>Lower cost (fewer API round trips)</li> </ul>"},{"location":"architecture/#extension-points","title":"Extension Points","text":""},{"location":"architecture/#custom-storage-backend","title":"Custom Storage Backend","text":"<p>Replace SQLite with another backend:</p> <pre><code>class RedisStore:\n    \"\"\"Custom storage using Redis.\"\"\"\n\n    def fetch_headline(self, mark_used=True, allow_used=False):\n        # Implement using Redis\n        pass\n\n# Use custom store\nprovider = NewsProvider(fake)\nprovider.store = RedisStore()\n</code></pre>"},{"location":"architecture/#custom-llm-provider","title":"Custom LLM Provider","text":"<p>Use a different LLM library:</p> <pre><code>class CustomLLMClient:\n    \"\"\"Custom LLM client using different library.\"\"\"\n\n    def generate_headlines(self, count):\n        # Use custom implementation\n        pass\n\nprovider = NewsProvider(fake)\nprovider.client = CustomLLMClient()\n</code></pre>"},{"location":"architecture/#custom-content-types","title":"Custom Content Types","text":"<p>Extend to generate other content:</p> <pre><code>class ExtendedProvider(NewsProvider):\n    def news_summary(self, headline=None):\n        \"\"\"Generate article summaries.\"\"\"\n        # Implementation\n        pass\n</code></pre>"},{"location":"architecture/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Complete API documentation</li> <li>Contributing - Contributing to the codebase</li> </ul>"},{"location":"cache-management/","title":"Cache Management","text":"<p>faker-news uses a SQLite database to cache generated content. This guide covers advanced cache management strategies.</p>"},{"location":"cache-management/#understanding-the-cache","title":"Understanding the Cache","text":""},{"location":"cache-management/#cache-structure","title":"Cache Structure","text":"<p>The cache stores three types of content:</p> <ul> <li>Headlines: Short news headlines</li> <li>Intros: Article introductions (linked to headlines)</li> <li>Articles: Full article bodies (linked to headlines)</li> </ul> <p>Each item tracks three usage flags:</p> <ul> <li><code>used_headline</code>: Whether the headline has been consumed</li> <li><code>used_intro</code>: Whether the intro has been consumed</li> <li><code>used_article</code>: Whether the article has been consumed</li> </ul>"},{"location":"cache-management/#cache-location","title":"Cache Location","text":"<p>Default cache locations by platform:</p> Platform Location Linux <code>~/.cache/faker-news/cache.sqlite3</code> macOS <code>~/Library/Caches/faker-news/cache.sqlite3</code> Windows <code>%LOCALAPPDATA%\\faker-news\\cache\\cache.sqlite3</code>"},{"location":"cache-management/#viewing-cache-statistics","title":"Viewing Cache Statistics","text":""},{"location":"cache-management/#using-python","title":"Using Python","text":"<pre><code>from faker import Faker\nfrom faker_news import NewsProvider\n\nfake = Faker()\nfake.add_provider(NewsProvider(fake))\n\nstats = fake.news_stats()\nprint(stats)\n# {\n#   'total': 150,\n#   'with_intro': 75,\n#   'with_article': 50,\n#   'unused_headlines': 100,\n#   'unused_intros': 60,\n#   'unused_articles': 40\n# }\n</code></pre>"},{"location":"cache-management/#using-cli","title":"Using CLI","text":"<pre><code>faker-news stats\n</code></pre>"},{"location":"cache-management/#understanding-the-statistics","title":"Understanding the Statistics","text":"Stat Description <code>total</code> Total number of items (headlines) in cache <code>with_intro</code> Number of headlines that have intros <code>with_article</code> Number of headlines that have articles <code>unused_headlines</code> Headlines not marked as used <code>unused_intros</code> Intros not marked as used <code>unused_articles</code> Articles not marked as used"},{"location":"cache-management/#consumption-strategies","title":"Consumption Strategies","text":""},{"location":"cache-management/#default-single-use","title":"Default: Single-Use","text":"<p>By default, items are marked as \"used\" after fetching (in Python):</p> <pre><code># Each fetch marks item as used\nheadline1 = fake.news_headline()  # Marks as used\nheadline2 = fake.news_headline()  # Different headline\n</code></pre> <p>CLI Difference</p> <p>CLI commands do NOT mark items as used by default. Use <code>--consume</code> flag to mark as used.</p>"},{"location":"cache-management/#strategy-1-non-consuming-fetches","title":"Strategy 1: Non-Consuming Fetches","text":"<p>Fetch items without marking as used (useful for testing):</p> <pre><code># View content without depleting pool\nheadline = fake.news_headline(consume=False)\narticle = fake.news_article(consume=False)\n\n# Same item might be fetched multiple times\n</code></pre>"},{"location":"cache-management/#strategy-2-allow-used-items","title":"Strategy 2: Allow Used Items","text":"<p>Fetch from the entire pool (both used and unused):</p> <pre><code># Fetch from any item\nheadline = fake.news_headline(allow_used=True)\n\n# Combine: fetch from any pool without consuming\nheadline = fake.news_headline(allow_used=True, consume=False)\n</code></pre>"},{"location":"cache-management/#strategy-3-reset-and-reuse","title":"Strategy 3: Reset and Reuse","text":"<p>Mark all items as unused:</p> <pre><code># Mark everything as unused\nfake.news_reset(\"reuse\")\n\n# Now all items are available again\nheadline = fake.news_headline()  # Can fetch any item\n</code></pre> <p>Or via CLI:</p> <pre><code>faker-news reset --mode reuse\n</code></pre>"},{"location":"cache-management/#preloading-strategies","title":"Preloading Strategies","text":""},{"location":"cache-management/#basic-preload","title":"Basic Preload","text":"<p>Generate headlines only:</p> <pre><code># Generate 100 headlines\nfake.news_preload_headlines(100)\n\n# Fetching is now instant (no API calls)\nheadlines = [fake.news_headline() for _ in range(50)]\n</code></pre>"},{"location":"cache-management/#full-content-preload","title":"Full Content Preload","text":"<p>Generate complete articles (less efficient):</p> <pre><code># Generate 50 new items with full content\nfaker-news preload --n 50 --with-intros --with-articles\n</code></pre> <p>Not Recommended for Large Batches</p> <p>This generates N new items even if you have unused items. Use populate mode instead.</p>"},{"location":"cache-management/#smart-populate-recommended","title":"Smart Populate (Recommended)","text":"<p>Ensure N unused items exist with full content:</p> <pre><code># Ensure 50 unused articles exist\nfaker-news preload --n 50 --populate\n</code></pre> <p>This is more efficient because it:</p> <ol> <li>Prioritizes unused headlines missing intros/articles</li> <li>Reuses existing complete unused headlines</li> <li>Generates new headlines only if needed</li> <li>Minimizes API calls by using what's already there</li> </ol> <p>Example:</p> <pre><code># Current cache: 30 unused headlines, 10 with full content\n\n# Smart populate to ensure 50 with full content\nfake.news_populate_headlines(50)\n\n# Result:\n# - 10 items already complete (reused)\n# - 20 items get intros/articles generated\n# - 20 new headlines generated with full content\n# Total API calls: 20 intros + 20 articles + 20 headlines = 60\n# vs. 150 if we generated 50 from scratch\n</code></pre>"},{"location":"cache-management/#resetting-the-cache","title":"Resetting the Cache","text":""},{"location":"cache-management/#reuse-content","title":"Reuse Content","text":"<p>Mark all items as unused (keep content):</p> <pre><code>fake.news_reset(\"reuse\")\n</code></pre> <pre><code>faker-news reset --mode reuse\n</code></pre> <p>When to use:</p> <ul> <li>You've consumed all content but want to reuse it</li> <li>Testing scenarios where you need repeatable data</li> <li>Cycling through content multiple times</li> </ul>"},{"location":"cache-management/#clear-content","title":"Clear Content","text":"<p>Delete all cached items:</p> <pre><code>fake.news_reset(\"clear\")\n</code></pre> <pre><code>faker-news reset --mode clear\n</code></pre> <p>When to use:</p> <ul> <li>Starting fresh with new content</li> <li>Cache has stale/unwanted content</li> <li>Freeing up disk space</li> </ul>"},{"location":"cache-management/#advanced-techniques","title":"Advanced Techniques","text":""},{"location":"cache-management/#maintain-a-ready-to-use-pool","title":"Maintain a Ready-to-Use Pool","text":"<p>Keep a pool of N complete articles ready:</p> <pre><code>from faker import Faker\nfrom faker_news import NewsProvider\n\ndef ensure_article_pool(size=50):\n    \"\"\"Ensure at least 'size' unused articles exist.\"\"\"\n    fake = Faker()\n    fake.add_provider(NewsProvider(fake))\n\n    stats = fake.news_stats()\n    current = stats['unused_articles']\n\n    if current &lt; size:\n        needed = size - current\n        print(f\"Generating {needed} articles...\")\n\n        # Use populate mode\n        # (This is conceptual - not in current API)\n        # For now, use preload with populate flag via CLI\n\nensure_article_pool(100)\n</code></pre>"},{"location":"cache-management/#separate-caches-for-different-use-cases","title":"Separate Caches for Different Use Cases","text":"<p>Use different databases for different purposes:</p> <pre><code>from faker import Faker\nfrom faker_news import NewsProvider\n\n# Production cache\nprod_provider = NewsProvider(fake, db_path=\"/var/cache/faker-news.sqlite3\")\n\n# Test cache\ntest_provider = NewsProvider(fake, db_path=\"/tmp/test-news.sqlite3\")\n\n# Development cache (default location)\ndev_provider = NewsProvider(fake)\n</code></pre>"},{"location":"cache-management/#batch-processing","title":"Batch Processing","text":"<p>Process content in batches efficiently:</p> <pre><code>from faker import Faker\nfrom faker_news import NewsProvider\n\ndef generate_batch(count=100):\n    \"\"\"Generate a batch of complete articles.\"\"\"\n    fake = Faker()\n    fake.add_provider(NewsProvider(fake))\n\n    # Preload headlines\n    fake.news_preload_headlines(count)\n\n    articles = []\n    for i in range(count):\n        headline = fake.news_headline()\n        intro = fake.news_intro(headline=headline)\n        article = fake.news_article(headline=headline, words=500)\n\n        articles.append({\n            'headline': headline,\n            'intro': intro,\n            'article': article\n        })\n\n        if (i + 1) % 10 == 0:\n            print(f\"Generated {i + 1}/{count}...\")\n\n    return articles\n\n# Generate 100 complete articles\narticles = generate_batch(100)\n</code></pre>"},{"location":"cache-management/#monitoring-cache-growth","title":"Monitoring Cache Growth","text":"<p>Track how the cache grows over time:</p> <pre><code>import time\nfrom faker import Faker\nfrom faker_news import NewsProvider\n\nfake = Faker()\nfake.add_provider(NewsProvider(fake))\n\n# Initial state\nprint(\"Initial:\", fake.news_stats())\n\n# Generate content\nfor i in range(50):\n    fake.news_headline()\n    if i % 10 == 0:\n        stats = fake.news_stats()\n        print(f\"After {i} fetches:\", stats)\n        time.sleep(0.1)\n</code></pre>"},{"location":"cache-management/#custom-database-path","title":"Custom Database Path","text":""},{"location":"cache-management/#python","title":"Python","text":"<pre><code>from faker import Faker\nfrom faker_news import NewsProvider\n\n# Custom location\nprovider = NewsProvider(fake, db_path=\"/custom/path/cache.sqlite3\")\nfake.add_provider(provider)\n</code></pre>"},{"location":"cache-management/#cli","title":"CLI","text":"<pre><code># Use custom database for all operations\nfaker-news preload --n 50 --db /custom/path/cache.sqlite3\nfaker-news headline --db /custom/path/cache.sqlite3\nfaker-news stats --db /custom/path/cache.sqlite3\n</code></pre>"},{"location":"cache-management/#environment-variable","title":"Environment Variable","text":"<p>Set a default custom path (not currently implemented, but could be):</p> <pre><code>export FAKER_NEWS_DB=\"/custom/path/cache.sqlite3\"\nfaker-news headline  # Uses custom path\n</code></pre>"},{"location":"cache-management/#cache-maintenance","title":"Cache Maintenance","text":""},{"location":"cache-management/#checking-cache-size","title":"Checking Cache Size","text":"<pre><code># Linux/macOS\ndu -h ~/.cache/faker-news/cache.sqlite3\n\n# Windows\ndir %LOCALAPPDATA%\\faker-news\\cache\\cache.sqlite3\n</code></pre>"},{"location":"cache-management/#backing-up-cache","title":"Backing Up Cache","text":"<pre><code># Linux/macOS\ncp ~/.cache/faker-news/cache.sqlite3 ~/backup-cache.sqlite3\n\n# Restore\ncp ~/backup-cache.sqlite3 ~/.cache/faker-news/cache.sqlite3\n</code></pre>"},{"location":"cache-management/#vacuuming-database","title":"Vacuuming Database","text":"<p>After many deletions, vacuum the database to reclaim space:</p> <pre><code>import sqlite3\nfrom pathlib import Path\nfrom platformdirs import user_cache_dir\n\ncache_dir = Path(user_cache_dir(\"faker-news\"))\ndb_path = cache_dir / \"cache.sqlite3\"\n\nconn = sqlite3.connect(db_path)\nconn.execute(\"VACUUM\")\nconn.close()\n</code></pre>"},{"location":"cache-management/#best-practices","title":"Best Practices","text":"<ol> <li>Preload before batch operations - Preload headlines before generating many articles</li> <li>Use populate mode - More efficient than generating from scratch</li> <li>Monitor your pool - Check stats regularly to ensure enough unused items</li> <li>Reset periodically - Use <code>reuse</code> mode to cycle through content</li> <li>Separate environments - Use different databases for prod/dev/test</li> <li>Backup important caches - If you've generated a lot of content</li> </ol>"},{"location":"cache-management/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration - Customize batch sizes and thresholds</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"cli-reference/","title":"CLI Reference","text":"<p>Complete reference for the faker-news command-line interface.</p>"},{"location":"cli-reference/#global-options","title":"Global Options","text":"<p>All commands support these global options:</p> Option Description <code>--help</code> Show help message and exit <code>--db PATH</code> Custom database file path"},{"location":"cli-reference/#commands","title":"Commands","text":""},{"location":"cli-reference/#setup","title":"setup","text":"<p>Interactive setup wizard for configuring API keys.</p> <pre><code>faker-news setup\n</code></pre> <p>What it does:</p> <ol> <li>Checks for existing API keys (keyring and environment)</li> <li>Guides you through provider selection</li> <li>Securely stores your API key in system keyring</li> <li>Tests the connection with a sample generation</li> <li>Confirms everything works</li> </ol> <p>Example:</p> <pre><code>$ faker-news setup\nChecking for API keys...\n\u2713 Found API key in keyring\n\nTesting connection...\n\u2713 Successfully generated test headline\n\nYour setup is complete!\n</code></pre>"},{"location":"cli-reference/#headline","title":"headline","text":"<p>Generate a fake news headline.</p> <pre><code>faker-news headline [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--consume</code> Mark the headline as used <code>False</code> <code>--allow-used</code> Fetch from all items (used or unused) <code>False</code> <code>--new</code> Always generate a new headline (skip cache) <code>False</code> <code>--db PATH</code> Custom database path Platform default <p>Examples:</p> <pre><code># Generate a headline (can repeat)\nfaker-news headline\n\n# Generate and mark as used\nfaker-news headline --consume\n\n# Always generate a fresh headline\nfaker-news headline --new\n\n# Generate multiple variations\nfaker-news headline --new  # First variation\nfaker-news headline --new  # Different variation\n\n# Fetch from all items (including used ones)\nfaker-news headline --allow-used\n\n# Use custom database\nfaker-news headline --db /tmp/cache.sqlite3\n</code></pre>"},{"location":"cli-reference/#intro","title":"intro","text":"<p>Generate a fake news article introduction.</p> <pre><code>faker-news intro [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--headline TEXT</code> Specific headline to use Auto-generated <code>--consume</code> Mark the intro as used <code>False</code> <code>--allow-used</code> Fetch from all items <code>False</code> <code>--new</code> Always generate a new intro (skip cache) <code>False</code> <code>--db PATH</code> Custom database path Platform default <p>Examples:</p> <pre><code># Generate an intro (with auto-generated headline)\nfaker-news intro\n\n# Generate intro for specific headline\nfaker-news intro --headline \"Scientists Make Breakthrough Discovery\"\n\n# Always generate a fresh intro\nfaker-news intro --new\n\n# Generate new intro for existing headline\nfaker-news intro --headline \"Breaking News\" --new\n\n# Generate and consume\nfaker-news intro --consume\n\n# Use with custom headline and consume\nfaker-news intro --headline \"Breaking News\" --consume\n</code></pre>"},{"location":"cli-reference/#article","title":"article","text":"<p>Generate a complete fake news article with markdown formatting.</p> <pre><code>faker-news article [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--headline TEXT</code> Specific headline to use Auto-generated <code>--words INT</code> Target article length in words <code>500</code> <code>--consume</code> Mark the article as used <code>False</code> <code>--allow-used</code> Fetch from all items <code>False</code> <code>--longest</code> Fetch the longest available article <code>False</code> <code>--new</code> Always generate a new article (skip cache) <code>False</code> <code>--db PATH</code> Custom database path Platform default <p>Examples:</p> <pre><code># Generate a ~500 word article\nfaker-news article\n\n# Generate a longer article\nfaker-news article --words 1000\n\n# Always generate a fresh article\nfaker-news article --new\n\n# Generate multiple variations\nfaker-news article --new --words 800  # First variation\nfaker-news article --new --words 800  # Different variation\n\n# Generate for specific headline\nfaker-news article --headline \"Tech Giant Announces New Product\"\n\n# Generate new article for existing headline\nfaker-news article --headline \"Tech Giant Announces New Product\" --new\n\n# Generate, consume, and use custom length\nfaker-news article --words 800 --consume\n\n# Fetch from any item (including used)\nfaker-news article --allow-used\n\n# Get the longest available article\nfaker-news article --longest\n</code></pre>"},{"location":"cli-reference/#preload","title":"preload","text":"<p>Preload the cache with headlines and optionally full content.</p> <pre><code>faker-news preload [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--n INT</code> Number of headlines to generate Required <code>--with-intros</code> Also generate intros <code>False</code> <code>--with-articles</code> Also generate articles <code>False</code> <code>--words INT</code> Article length (if generating) <code>500</code> <code>--populate</code> Smart populate mode <code>False</code> <code>--db PATH</code> Custom database path Platform default <p>Modes:</p> Basic PreloadPreload with ContentPopulate Mode <p>Generate N new headlines only:</p> <pre><code>faker-news preload --n 50\n</code></pre> <p>Generate N new headlines with full content:</p> <pre><code>faker-news preload --n 50 --with-intros --with-articles\n</code></pre> <p>Ensure N unused headlines exist with full content (smart mode):</p> <pre><code>faker-news preload --n 50 --populate\n</code></pre> <p>This mode:</p> <ul> <li>Prioritizes unused headlines missing intros/articles</li> <li>Uses existing complete unused headlines if available</li> <li>Generates new headlines only if needed</li> <li>Minimizes API calls</li> </ul> <p>Examples:</p> <pre><code># Generate 100 headlines\nfaker-news preload --n 100\n\n# Generate 50 complete articles\nfaker-news preload --n 50 --with-intros --with-articles\n\n# Generate longer articles\nfaker-news preload --n 20 --with-articles --words 1000\n\n# Smart populate: ensure 50 ready-to-use articles exist\nfaker-news preload --n 50 --populate\n\n# Populate with custom article length\nfaker-news preload --n 50 --populate --words 800\n</code></pre>"},{"location":"cli-reference/#stats","title":"stats","text":"<p>Display cache statistics.</p> <pre><code>faker-news stats [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--db PATH</code> Custom database path Platform default <p>Example:</p> <pre><code>$ faker-news stats\nCache Statistics:\n  Total items: 150\n  With intros: 75\n  With articles: 50\n  Unused headlines: 100\n  Unused intros: 60\n  Unused articles: 40\n</code></pre>"},{"location":"cli-reference/#reset","title":"reset","text":"<p>Reset usage flags or clear the cache entirely.</p> <pre><code>faker-news reset [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--mode MODE</code> Reset mode: <code>reuse</code> or <code>clear</code> Required <code>--db PATH</code> Custom database path Platform default <p>Modes:</p> ReuseClear <p>Mark all items as unused (keep content):</p> <pre><code>faker-news reset --mode reuse\n</code></pre> <p>This marks all headlines, intros, and articles as \"unused\" so they can be fetched again. The content remains in the cache.</p> <p>Delete all cached content:</p> <pre><code>faker-news reset --mode clear\n</code></pre> <p>This completely empties the cache database. Next fetch will trigger new generation.</p> <p>Examples:</p> <pre><code># Mark all items as unused\nfaker-news reset --mode reuse\n\n# Clear all content\nfaker-news reset --mode clear\n\n# Reuse with custom database\nfaker-news reset --mode reuse --db /tmp/cache.sqlite3\n</code></pre>"},{"location":"cli-reference/#common-workflows","title":"Common Workflows","text":""},{"location":"cli-reference/#first-time-setup","title":"First-Time Setup","text":"<pre><code># 1. Run interactive setup\nfaker-news setup\n\n# 2. Preload content for better performance\nfaker-news preload --n 100\n\n# 3. Check what's cached\nfaker-news stats\n</code></pre>"},{"location":"cli-reference/#generate-complete-articles","title":"Generate Complete Articles","text":"<pre><code># Ensure cache is populated\nfaker-news preload --n 50 --populate\n\n# Generate a complete article\nheadline=$(faker-news headline --consume)\nfaker-news intro --headline \"$headline\" --consume\nfaker-news article --headline \"$headline\" --consume\n</code></pre>"},{"location":"cli-reference/#testing-without-consuming","title":"Testing Without Consuming","text":"<pre><code># View content without marking as used\nfaker-news headline\nfaker-news intro\nfaker-news article\n\n# Same command can be run multiple times\nfaker-news headline  # Might show same headline\nfaker-news headline  # Might show same headline\n</code></pre>"},{"location":"cli-reference/#generating-multiple-variations","title":"Generating Multiple Variations","text":"<p>Use the <code>--new</code> flag to generate fresh content every time, bypassing the cache:</p> <pre><code># Generate 5 different headlines\nfor i in {1..5}; do\n  faker-news headline --new\ndone\n\n# Generate 3 different article variations for testing\nfaker-news article --new --words 500 &gt; article1.txt\nfaker-news article --new --words 500 &gt; article2.txt\nfaker-news article --new --words 500 &gt; article3.txt\n\n# Create multiple intros for the same headline\nheadline=\"Breaking: Scientists Discover New Planet\"\nfaker-news intro --headline \"$headline\" --new &gt; intro1.txt\nfaker-news intro --headline \"$headline\" --new &gt; intro2.txt\nfaker-news intro --headline \"$headline\" --new &gt; intro3.txt\n</code></pre>"},{"location":"cli-reference/#batch-generation-script","title":"Batch Generation Script","text":"<pre><code>#!/bin/bash\n# Generate 20 complete articles\n\nfaker-news preload --n 20 --populate\n\nfor i in {1..20}; do\n  headline=$(faker-news headline --consume)\n  intro=$(faker-news intro --headline \"$headline\" --consume)\n  article=$(faker-news article --headline \"$headline\" --consume)\n\n  echo \"===== Article $i =====\"\n  echo \"$headline\"\n  echo \"\"\n  echo \"$intro\"\n  echo \"\"\n  echo \"$article\"\n  echo \"\"\n  echo \"\"\ndone\n</code></pre>"},{"location":"cli-reference/#reset-and-start-fresh","title":"Reset and Start Fresh","text":"<pre><code># Option 1: Reuse existing content\nfaker-news reset --mode reuse\nfaker-news stats  # All items now unused\n\n# Option 2: Clear everything and start over\nfaker-news reset --mode clear\nfaker-news preload --n 50 --populate\nfaker-news stats\n</code></pre>"},{"location":"cli-reference/#exit-codes","title":"Exit Codes","text":"Code Meaning <code>0</code> Success <code>1</code> General error (API error, missing config, etc.) <code>2</code> Invalid arguments"},{"location":"cli-reference/#using-with-scripts","title":"Using with Scripts","text":"<p>The CLI outputs only the generated content to stdout, making it easy to use in scripts:</p> <pre><code># Capture output\nheadline=$(faker-news headline)\necho \"Generated: $headline\"\n\n# Use in loops\nfor i in {1..5}; do\n  faker-news headline\ndone\n\n# Redirect to file\nfaker-news article --words 1000 &gt; article.txt\n</code></pre>"},{"location":"cli-reference/#next-steps","title":"Next Steps","text":"<ul> <li>Cache Management - Advanced cache management strategies</li> <li>Library Usage - Use faker-news in Python</li> <li>Configuration - Customize behavior</li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>This guide covers all configuration options for customizing faker-news behavior.</p>"},{"location":"configuration/#provider-configuration","title":"Provider Configuration","text":""},{"location":"configuration/#newsprovider-parameters","title":"NewsProvider Parameters","text":"<p>Configure the provider when creating it:</p> <pre><code>from faker import Faker\nfrom faker_news import NewsProvider, LLMClientConfig\n\nfake = Faker()\nprovider = NewsProvider(\n    fake,\n    db_path=\"/custom/path/cache.sqlite3\",  # Custom cache location\n    min_headline_pool=50,                   # Minimum unused headlines threshold\n    headline_batch=60,                      # Headlines per batch\n    intro_batch=30,                         # Intros per batch\n    article_batch=15,                       # Articles per batch\n    llm_config=None                         # LLM configuration (optional)\n)\nfake.add_provider(provider)\n</code></pre>"},{"location":"configuration/#configuration-parameters","title":"Configuration Parameters","text":"Parameter Type Default Description <code>db_path</code> <code>str</code> Platform-specific SQLite database file path <code>min_headline_pool</code> <code>int</code> <code>30</code> Minimum unused headlines before auto-refill <code>headline_batch</code> <code>int</code> <code>40</code> Headlines generated per batch <code>intro_batch</code> <code>int</code> <code>20</code> Intros generated per batch <code>article_batch</code> <code>int</code> <code>10</code> Articles generated per batch <code>llm_config</code> <code>LLMClientConfig</code> <code>None</code> LLM client configuration"},{"location":"configuration/#default-database-paths","title":"Default Database Paths","text":"<p>If <code>db_path</code> is not specified, platform-specific defaults are used:</p> Platform Default Path Linux <code>~/.cache/faker-news/cache.sqlite3</code> macOS <code>~/Library/Caches/faker-news/cache.sqlite3</code> Windows <code>%LOCALAPPDATA%\\faker-news\\cache\\cache.sqlite3</code>"},{"location":"configuration/#llm-configuration","title":"LLM Configuration","text":""},{"location":"configuration/#llmclientconfig-parameters","title":"LLMClientConfig Parameters","text":"<p>Configure the LLM client:</p> <pre><code>from faker_news import LLMClientConfig\n\nllm_config = LLMClientConfig(\n    api_key=\"sk-your-api-key\",              # API key\n    base_url=\"https://api.openai.com/v1\",   # API endpoint\n    model_headlines=\"gpt-4o-mini\",          # Model for headlines\n    model_writing=\"gpt-4o-mini\"             # Model for intros/articles\n)\n</code></pre> Parameter Type Default Description <code>api_key</code> <code>str</code> Auto-detected LLM API key <code>base_url</code> <code>str</code> Auto-detected API endpoint URL <code>model_headlines</code> <code>str</code> Provider-specific Model for headline generation <code>model_writing</code> <code>str</code> Provider-specific Model for intro/article generation"},{"location":"configuration/#auto-detection","title":"Auto-Detection","text":"<p>If not specified, values are auto-detected:</p> <ol> <li>API Key: Checked in order:</li> <li>System keyring (<code>keyring.get_password(\"faker-news\", \"openai\")</code>)</li> <li>Environment variable (<code>OPENAI_API_KEY</code> or <code>DASHSCOPE_API_KEY</code>)</li> <li> <p>Explicit config</p> </li> <li> <p>Base URL: Checked in order:</p> </li> <li>Environment variable (<code>OPENAI_BASE_URL</code> or <code>DASHSCOPE_BASE_URL</code>)</li> <li> <p>Default based on detected provider</p> </li> <li> <p>Models: Auto-selected based on provider:</p> </li> <li>OpenAI: <code>gpt-4o-mini</code></li> <li>DashScope/Qwen: <code>qwen-flash</code></li> </ol>"},{"location":"configuration/#configuration-examples","title":"Configuration Examples","text":""},{"location":"configuration/#development-setup","title":"Development Setup","text":"<p>Fast, cheap models for development:</p> <pre><code>from faker import Faker\nfrom faker_news import NewsProvider, LLMClientConfig\n\nllm_config = LLMClientConfig(\n    model_headlines=\"gpt-4o-mini\",  # Fast and cheap\n    model_writing=\"gpt-4o-mini\"\n)\n\nfake = Faker()\nprovider = NewsProvider(\n    fake,\n    llm_config=llm_config,\n    min_headline_pool=20,      # Lower threshold\n    headline_batch=30,         # Smaller batches\n    db_path=\"/tmp/dev-cache.sqlite3\"\n)\nfake.add_provider(provider)\n</code></pre>"},{"location":"configuration/#production-setup","title":"Production Setup","text":"<p>Better models for production:</p> <pre><code>llm_config = LLMClientConfig(\n    model_headlines=\"gpt-4o-mini\",  # Fast for headlines\n    model_writing=\"gpt-4o\"          # Better quality for articles\n)\n\nfake = Faker()\nprovider = NewsProvider(\n    fake,\n    llm_config=llm_config,\n    min_headline_pool=100,     # Larger buffer\n    headline_batch=150,        # Bigger batches\n    db_path=\"/var/cache/faker-news/cache.sqlite3\"\n)\nfake.add_provider(provider)\n</code></pre>"},{"location":"configuration/#high-volume-setup","title":"High-Volume Setup","text":"<p>Optimized for generating lots of content:</p> <pre><code>provider = NewsProvider(\n    fake,\n    min_headline_pool=200,     # Large buffer\n    headline_batch=250,        # Large batches\n    intro_batch=50,\n    article_batch=25,\n    db_path=\"/data/faker-news-cache.sqlite3\"\n)\n</code></pre>"},{"location":"configuration/#testing-setup","title":"Testing Setup","text":"<p>Isolated environment for testing:</p> <pre><code>import tempfile\nfrom pathlib import Path\n\n# Use temporary database\ntemp_db = Path(tempfile.mkdtemp()) / \"test-cache.sqlite3\"\n\nprovider = NewsProvider(\n    fake,\n    db_path=str(temp_db),\n    min_headline_pool=10,      # Small for tests\n    headline_batch=15\n)\n</code></pre>"},{"location":"configuration/#multi-model-setup","title":"Multi-Model Setup","text":"<p>Use different models for different content types:</p> <pre><code>llm_config = LLMClientConfig(\n    api_key=\"sk-...\",\n    model_headlines=\"gpt-4o-mini\",  # Fast, simple model for headlines\n    model_writing=\"gpt-4o\"          # Better model for full articles\n)\n\nprovider = NewsProvider(fake, llm_config=llm_config)\n</code></pre>"},{"location":"configuration/#tuning-performance","title":"Tuning Performance","text":""},{"location":"configuration/#batch-sizes","title":"Batch Sizes","text":"<p>Larger batches = fewer API calls but longer waits:</p> Use Case Batch Size Pros Cons Development 20-40 Fast feedback More API calls Production 100-200 Fewer API calls Longer waits High-volume 200+ Optimal efficiency Memory usage <pre><code># Conservative (more API calls, but faster feedback)\nprovider = NewsProvider(\n    fake,\n    headline_batch=30,\n    intro_batch=15,\n    article_batch=8\n)\n\n# Aggressive (fewer API calls, but longer waits)\nprovider = NewsProvider(\n    fake,\n    headline_batch=200,\n    intro_batch=50,\n    article_batch=30\n)\n</code></pre>"},{"location":"configuration/#pool-threshold","title":"Pool Threshold","text":"<p>Controls when auto-refill happens:</p> <pre><code># Low threshold (more frequent refills)\nprovider = NewsProvider(fake, min_headline_pool=10)\n# Refills when &lt; 10 unused headlines\n\n# High threshold (less frequent refills, but larger buffer)\nprovider = NewsProvider(fake, min_headline_pool=100)\n# Refills when &lt; 100 unused headlines\n</code></pre> <p>Recommendations:</p> <ul> <li>Interactive use: Low threshold (10-30)</li> <li>Batch processing: High threshold (50-200)</li> <li>Background jobs: Very high threshold (200+)</li> </ul>"},{"location":"configuration/#memory-considerations","title":"Memory Considerations","text":"<p>Large batches use more memory during generation:</p> <pre><code># Memory-efficient (smaller batches)\nprovider = NewsProvider(\n    fake,\n    headline_batch=40,\n    article_batch=10\n)\n\n# Memory-intensive (larger batches)\nprovider = NewsProvider(\n    fake,\n    headline_batch=500,\n    article_batch=100\n)\n</code></pre>"},{"location":"configuration/#provider-specific-configuration","title":"Provider-Specific Configuration","text":""},{"location":"configuration/#openai","title":"OpenAI","text":"<pre><code>llm_config = LLMClientConfig(\n    api_key=\"sk-...\",\n    base_url=\"https://api.openai.com/v1\",\n    model_headlines=\"gpt-4o-mini\",\n    model_writing=\"gpt-4o\"\n)\n</code></pre>"},{"location":"configuration/#alibaba-dashscopeqwen","title":"Alibaba DashScope/Qwen","text":"<pre><code>llm_config = LLMClientConfig(\n    api_key=\"sk-...\",\n    base_url=\"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\",\n    model_headlines=\"qwen-flash\",   # Fast model\n    model_writing=\"qwen-plus\"       # Better quality\n)\n</code></pre>"},{"location":"configuration/#azure-openai","title":"Azure OpenAI","text":"<pre><code>llm_config = LLMClientConfig(\n    api_key=\"your-azure-key\",\n    base_url=\"https://your-resource.openai.azure.com/openai/deployments/your-deployment\",\n    model_headlines=\"gpt-4o-mini\",\n    model_writing=\"gpt-4\"\n)\n</code></pre>"},{"location":"configuration/#custom-openai-compatible-api","title":"Custom OpenAI-Compatible API","text":"<pre><code>llm_config = LLMClientConfig(\n    api_key=\"your-api-key\",\n    base_url=\"https://custom-api.example.com/v1\",\n    model_headlines=\"custom-model-fast\",\n    model_writing=\"custom-model-quality\"\n)\n</code></pre>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"configuration/#setting-credentials","title":"Setting Credentials","text":"<p>Instead of hardcoding, use environment variables:</p> <pre><code># OpenAI\nexport OPENAI_API_KEY=\"sk-...\"\nexport OPENAI_BASE_URL=\"https://api.openai.com/v1\"  # Optional\n\n# DashScope\nexport DASHSCOPE_API_KEY=\"sk-...\"\nexport DASHSCOPE_BASE_URL=\"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\"  # Optional\n</code></pre> <p>Then use without explicit config:</p> <pre><code>from faker import Faker\nfrom faker_news import NewsProvider\n\n# Auto-detects from environment\nfake = Faker()\nfake.add_provider(NewsProvider(fake))\n</code></pre>"},{"location":"configuration/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"configuration/#multiple-providers","title":"Multiple Providers","text":"<p>Use different configurations for different purposes:</p> <pre><code>from faker import Faker\nfrom faker_news import NewsProvider, LLMClientConfig\n\n# Fast provider for headlines only\nfast_config = LLMClientConfig(model_headlines=\"gpt-4o-mini\")\nfast_provider = NewsProvider(\n    Faker(),\n    llm_config=fast_config,\n    db_path=\"/tmp/headlines.sqlite3\"\n)\n\n# Quality provider for articles\nquality_config = LLMClientConfig(model_writing=\"gpt-4o\")\nquality_provider = NewsProvider(\n    Faker(),\n    llm_config=quality_config,\n    db_path=\"/tmp/articles.sqlite3\"\n)\n</code></pre>"},{"location":"configuration/#dynamic-configuration","title":"Dynamic Configuration","text":"<p>Adjust configuration at runtime:</p> <pre><code>def create_provider(env=\"development\"):\n    \"\"\"Create provider based on environment.\"\"\"\n    if env == \"production\":\n        return NewsProvider(\n            fake,\n            min_headline_pool=100,\n            headline_batch=200,\n            db_path=\"/var/cache/faker-news.sqlite3\"\n        )\n    else:\n        return NewsProvider(\n            fake,\n            min_headline_pool=20,\n            headline_batch=30,\n            db_path=\"/tmp/dev-cache.sqlite3\"\n        )\n\n# Use it\nprovider = create_provider(env=\"production\")\nfake.add_provider(provider)\n</code></pre>"},{"location":"configuration/#next-steps","title":"Next Steps","text":"<ul> <li>LLM Providers - Detailed guides for specific LLM providers</li> <li>API Reference - Complete API documentation</li> <li>Architecture - Understanding the internal architecture</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to faker-news! This guide will help you get started.</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/smileychris/faker-news.git\ncd faker-news\n</code></pre>"},{"location":"contributing/#2-install-dependencies","title":"2. Install Dependencies","text":"<p>Install the package in development mode with all dependencies:</p> <pre><code>pip install -e \".[dev]\"\n</code></pre> <p>This installs: - Core dependencies (faker, openai, click, etc.) - Development tools (pytest, ruff) - Documentation tools (mkdocs, mkdocs-material)</p>"},{"location":"contributing/#3-configure-api-key","title":"3. Configure API Key","text":"<p>Run the setup wizard to configure your LLM API key:</p> <pre><code>uv run faker-news setup\n</code></pre> <p>Or set environment variables:</p> <pre><code>export OPENAI_API_KEY=\"your-api-key\"\n</code></pre>"},{"location":"contributing/#4-verify-installation","title":"4. Verify Installation","text":"<p>Run the tests to verify everything works:</p> <pre><code>uv run pytest\n</code></pre>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nuv run pytest\n\n# Run with coverage\nuv run pytest --cov=faker_news --cov-report=html\n\n# Run specific test file\nuv run pytest tests/test_provider.py\n\n# Run verbose\nuv run pytest -v\n\n# Skip slow/integration tests\nuv run pytest -m \"not integration and not slow\"\n</code></pre>"},{"location":"contributing/#code-formatting","title":"Code Formatting","text":"<p>We use Ruff for code formatting:</p> <pre><code># Format all code\nruff format src/ tests/\n\n# Check without modifying\nruff format --check src/ tests/\n</code></pre>"},{"location":"contributing/#linting","title":"Linting","text":"<p>We use Ruff for linting:</p> <pre><code># Lint code\nruff check src/ tests/\n\n# Auto-fix issues\nruff check --fix src/ tests/\n</code></pre>"},{"location":"contributing/#building-documentation","title":"Building Documentation","text":"<p>Build and preview the documentation locally:</p> <pre><code># Install docs dependencies\npip install -e \".[docs]\"\n\n# Serve docs locally\nmkdocs serve\n\n# Build docs\nmkdocs build\n</code></pre> <p>Then visit http://localhost:8000 to view the docs.</p>"},{"location":"contributing/#code-guidelines","title":"Code Guidelines","text":""},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8 conventions</li> <li>Use type hints for all public APIs</li> <li>Maximum line length: 120 characters</li> <li>Use Ruff for formatting (automated)</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Add docstrings to all public functions/classes</li> <li>Include examples in docstrings</li> <li>Update relevant documentation when changing APIs</li> <li>IMPORTANT: When changing APIs, update:</li> <li><code>docs/api-reference.md</code> - For Python API changes</li> <li><code>docs/cli-reference.md</code> - For CLI command changes</li> <li>Other relevant docs as needed</li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<ul> <li>Write tests for all new features</li> <li>Maintain or improve code coverage</li> <li>Use pytest fixtures for common setup</li> <li>Mark slow tests with <code>@pytest.mark.slow</code></li> <li>Mark integration tests with <code>@pytest.mark.integration</code></li> </ul>"},{"location":"contributing/#example-test","title":"Example Test","text":"<pre><code>import pytest\nfrom faker import Faker\nfrom faker_news import NewsProvider\n\n\ndef test_news_headline(fake):\n    \"\"\"Test headline generation.\"\"\"\n    fake.add_provider(NewsProvider(fake))\n\n    headline = fake.news_headline()\n\n    assert isinstance(headline, str)\n    assert len(headline) &gt; 0\n\n\n@pytest.mark.integration\ndef test_full_article_generation(fake):\n    \"\"\"Test complete article generation (requires API key).\"\"\"\n    fake.add_provider(NewsProvider(fake))\n\n    headline = fake.news_headline()\n    intro = fake.news_intro(headline=headline)\n    article = fake.news_article(headline=headline)\n\n    assert headline in intro or intro.startswith(headline)\n    assert len(article) &gt; len(intro)\n</code></pre>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>faker-news/\n\u251c\u2500\u2500 src/faker_news/\n\u2502   \u251c\u2500\u2500 __init__.py       # Public API exports\n\u2502   \u251c\u2500\u2500 client.py         # LLMClient and LLMClientConfig\n\u2502   \u251c\u2500\u2500 store.py          # NewsStore (SQLite layer)\n\u2502   \u251c\u2500\u2500 provider.py       # NewsProvider (Faker provider)\n\u2502   \u251c\u2500\u2500 cli.py            # Command-line interface\n\u2502   \u2514\u2500\u2500 setup.py          # Interactive setup script\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 conftest.py       # Shared fixtures\n\u2502   \u251c\u2500\u2500 test_client.py    # LLMClient tests\n\u2502   \u251c\u2500\u2500 test_store.py     # NewsStore tests\n\u2502   \u251c\u2500\u2500 test_provider.py  # NewsProvider tests\n\u2502   \u2514\u2500\u2500 test_cli.py       # CLI tests\n\u251c\u2500\u2500 docs/                 # MkDocs documentation\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 getting-started.md\n\u2502   \u251c\u2500\u2500 api-reference.md\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 pyproject.toml        # Project configuration\n\u251c\u2500\u2500 mkdocs.yml            # Documentation configuration\n\u251c\u2500\u2500 README.md             # Quick reference\n\u2514\u2500\u2500 CLAUDE.md             # Developer guide (for Claude Code)\n</code></pre>"},{"location":"contributing/#making-changes","title":"Making Changes","text":""},{"location":"contributing/#1-create-a-branch","title":"1. Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"contributing/#2-make-your-changes","title":"2. Make Your Changes","text":"<ul> <li>Write code following the guidelines above</li> <li>Add tests for new functionality</li> <li>Update documentation if needed</li> <li>Format code with Black</li> <li>Lint code with Ruff</li> </ul>"},{"location":"contributing/#3-run-tests","title":"3. Run Tests","text":"<pre><code># Run all tests\nuv run pytest\n\n# Check coverage\nuv run pytest --cov=faker_news\n</code></pre>"},{"location":"contributing/#4-commit-your-changes","title":"4. Commit Your Changes","text":"<pre><code>git add .\ngit commit -m \"Add feature: your feature description\"\n</code></pre> <p>Use conventional commit messages: - <code>feat:</code> - New feature - <code>fix:</code> - Bug fix - <code>docs:</code> - Documentation changes - <code>test:</code> - Test changes - <code>refactor:</code> - Code refactoring - <code>chore:</code> - Maintenance tasks</p>"},{"location":"contributing/#5-push-and-create-pr","title":"5. Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a pull request on GitHub.</p>"},{"location":"contributing/#areas-for-contribution","title":"Areas for Contribution","text":""},{"location":"contributing/#features","title":"Features","text":"<ul> <li>Support for additional LLM providers</li> <li>New content types (summaries, captions, etc.)</li> <li>Advanced caching strategies</li> <li>Performance optimizations</li> <li>CLI enhancements</li> </ul>"},{"location":"contributing/#documentation_1","title":"Documentation","text":"<ul> <li>More usage examples</li> <li>Tutorials for specific use cases</li> <li>Translation to other languages</li> <li>Video walkthroughs</li> <li>API documentation improvements</li> </ul>"},{"location":"contributing/#testing_1","title":"Testing","text":"<ul> <li>Increase test coverage</li> <li>Add integration tests</li> <li>Performance benchmarks</li> <li>Stress tests</li> </ul>"},{"location":"contributing/#bug-fixes","title":"Bug Fixes","text":"<p>Check the issues page for reported bugs.</p>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":""},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>When reporting bugs, include:</p> <ol> <li>Description: What happened vs. what you expected</li> <li>Steps to reproduce: Exact steps to trigger the bug</li> <li>Environment:</li> <li>Python version</li> <li>faker-news version</li> <li>Operating system</li> <li>LLM provider being used</li> <li>Code sample: Minimal code to reproduce the issue</li> <li>Error messages: Full error output</li> </ol>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>When requesting features, include:</p> <ol> <li>Use case: Why you need this feature</li> <li>Proposed solution: How you envision it working</li> <li>Alternatives: Other approaches you've considered</li> <li>Examples: Similar features in other tools</li> </ol>"},{"location":"contributing/#code-review-process","title":"Code Review Process","text":"<ol> <li>Automated checks: CI runs tests and linting</li> <li>Manual review: Maintainer reviews code</li> <li>Feedback: Suggestions for improvements</li> <li>Approval: Once approved, PR is merged</li> </ol>"},{"location":"contributing/#release-process","title":"Release Process","text":"<p>Releases are managed by maintainers:</p> <ol> <li>Update version in <code>pyproject.toml</code></li> <li>Update CHANGELOG.md</li> <li>Create git tag</li> <li>Build distribution: <code>python -m build</code></li> <li>Publish to PyPI: <code>twine upload dist/*</code></li> <li>Create GitHub release</li> </ol>"},{"location":"contributing/#community-guidelines","title":"Community Guidelines","text":"<ul> <li>Be respectful and inclusive</li> <li>Help others in issues and discussions</li> <li>Follow the code of conduct</li> <li>Give credit where due</li> </ul>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> <li>Email: smileychris@gmail.com</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the MIT License.</p>"},{"location":"contributing/#recognition","title":"Recognition","text":"<p>Contributors are recognized in: - GitHub contributors page - Release notes - CONTRIBUTORS.md (coming soon)</p> <p>Thank you for contributing to faker-news! \ud83c\udf89</p>"},{"location":"getting-started/","title":"Installation","text":"<p>This guide will help you install faker-news and get it running on your system.</p>"},{"location":"getting-started/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9 or higher</li> <li>pip or uv package manager</li> <li>An API key for an OpenAI-compatible LLM service</li> </ul>"},{"location":"getting-started/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/#using-pip-standard","title":"Using pip (Standard)","text":"<p>Install the package directly from source:</p> <pre><code>pip install -e .\n</code></pre> <p>For development with testing tools:</p> <pre><code>pip install -e \".[dev]\"\n</code></pre> <p>For documentation building:</p> <pre><code>pip install -e \".[docs]\"\n</code></pre>"},{"location":"getting-started/#using-uv-recommended-for-development","title":"Using uv (Recommended for Development)","text":"<p>If you're using uv for faster package management:</p> <pre><code>uv pip install -e .\n</code></pre>"},{"location":"getting-started/#verifying-installation","title":"Verifying Installation","text":"<p>After installation, verify that the CLI is available:</p> <pre><code>faker-news --help\n</code></pre> <p>You should see the help menu with all available commands.</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Once installed, you'll need to configure your LLM API key. Continue to:</p> <ol> <li>Setup &amp; Configuration - Configure your API keys</li> <li>Quick Start - Start generating content right away</li> </ol>"},{"location":"getting-started/#uninstallation","title":"Uninstallation","text":"<p>To remove faker-news:</p> <pre><code>pip uninstall faker-news\n</code></pre> <p>Cache Files</p> <p>Uninstalling won't remove the SQLite cache file. To completely remove all data:</p> <pre><code># On Linux\nrm ~/.cache/faker-news/cache.sqlite3\n\n# On macOS\nrm ~/Library/Caches/faker-news/cache.sqlite3\n\n# On Windows\ndel %LOCALAPPDATA%\\faker-news\\cache\\cache.sqlite3\n</code></pre>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/#command-not-found","title":"Command not found","text":"<p>If <code>faker-news</code> command is not found after installation, ensure your Python scripts directory is in your PATH:</p> <pre><code># Add to ~/.bashrc or ~/.zshrc\nexport PATH=\"$HOME/.local/bin:$PATH\"\n</code></pre>"},{"location":"getting-started/#import-errors","title":"Import errors","text":"<p>If you get import errors when using the library, ensure you installed the package:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"getting-started/#permission-errors","title":"Permission errors","text":"<p>On some systems, you may need to install with <code>--user</code>:</p> <pre><code>pip install --user -e .\n</code></pre>"},{"location":"library-usage/","title":"Library Usage","text":"<p>This guide covers using faker-news as a Python library with the Faker framework.</p>"},{"location":"library-usage/#basic-setup","title":"Basic Setup","text":"<pre><code>from faker import Faker\nfrom faker_news import NewsProvider\n\n# Create Faker instance and add the provider\nfake = Faker()\nfake.add_provider(NewsProvider(fake))\n</code></pre>"},{"location":"library-usage/#generating-content","title":"Generating Content","text":""},{"location":"library-usage/#headlines","title":"Headlines","text":"<p>Generate standalone headlines:</p> <pre><code># Generate a random headline\nheadline = fake.news_headline()\nprint(headline)\n# \"Scientists Discover New Species in Deep Ocean\"\n\n# Generate multiple headlines\nheadlines = [fake.news_headline() for _ in range(5)]\n</code></pre>"},{"location":"library-usage/#introductions","title":"Introductions","text":"<p>Generate article introductions:</p> <pre><code># Generate intro for a specific headline\nheadline = \"Tech Company Launches Revolutionary Product\"\nintro = fake.news_intro(headline=headline)\nprint(intro)\n# \"In a major announcement today, the company revealed...\"\n\n# Generate standalone intro (with auto-generated headline)\nintro = fake.news_intro()\n</code></pre>"},{"location":"library-usage/#full-articles","title":"Full Articles","text":"<p>Generate complete articles with markdown formatting:</p> <pre><code># Generate a ~500-word article\nheadline = \"Climate Scientists Report Breakthrough Discovery\"\narticle = fake.news_article(headline=headline, words=500)\nprint(article)\n\n# Generate a longer article\nlong_article = fake.news_article(headline=headline, words=1000)\n\n# Generate standalone article (with auto-generated headline)\narticle = fake.news_article()\n</code></pre>"},{"location":"library-usage/#consumption-modes","title":"Consumption Modes","text":""},{"location":"library-usage/#default-behavior-consume","title":"Default Behavior (Consume)","text":"<p>By default, items are marked as \"used\" after fetching:</p> <pre><code># These mark items as used\nheadline1 = fake.news_headline()\nheadline2 = fake.news_headline()  # Different headline\nheadline3 = fake.news_headline()  # Different headline again\n\n# Once all unused headlines are consumed, new ones are auto-generated\n</code></pre>"},{"location":"library-usage/#non-consuming-mode","title":"Non-Consuming Mode","text":"<p>Fetch items without marking them as used:</p> <pre><code># Preview content without consuming\nheadline = fake.news_headline(consume=False)\nintro = fake.news_intro(consume=False)\narticle = fake.news_article(consume=False)\n\n# Useful for:\n# - Testing/debugging\n# - Previewing content before deciding to use it\n# - Showing examples without depleting the pool\n</code></pre>"},{"location":"library-usage/#allow-used-items","title":"Allow Used Items","text":"<p>Fetch from the entire pool (both used and unused):</p> <pre><code># Fetch from all items, not just unused\nheadline = fake.news_headline(allow_used=True)\narticle = fake.news_article(allow_used=True)\n\n# Combine with consume=False for true random sampling\nrandom_headline = fake.news_headline(allow_used=True, consume=False)\n</code></pre>"},{"location":"library-usage/#working-with-related-content","title":"Working with Related Content","text":"<p>Generate a complete article with matching headline, intro, and body:</p> <pre><code># Method 1: Generate piece by piece\nheadline = fake.news_headline()\nintro = fake.news_intro(headline=headline)\narticle = fake.news_article(headline=headline, words=600)\n\nprint(f\"{headline}\\n\\n{intro}\\n\\n{article}\")\n\n# Method 2: Generate all at once (more efficient)\nheadline = fake.news_headline()\n# Batch generate intro and article\nintro = fake.news_intro(headline=headline)\narticle = fake.news_article(headline=headline)\n</code></pre> <p>Performance Tip</p> <p>When generating multiple complete articles, preload headlines first:</p> <pre><code>fake.news_preload_headlines(100)\n\narticles = []\nfor _ in range(50):\n    headline = fake.news_headline()\n    intro = fake.news_intro(headline=headline)\n    article = fake.news_article(headline=headline)\n    articles.append((headline, intro, article))\n</code></pre>"},{"location":"library-usage/#preloading-content","title":"Preloading Content","text":"<p>For better performance, preload content in bulk:</p> <pre><code># Preload 100 headlines\nfake.news_preload_headlines(100)\n\n# Now fetching headlines is instant (no API calls)\nheadlines = [fake.news_headline() for _ in range(50)]\n\n# Check what's in the cache\nstats = fake.news_stats()\nprint(stats)\n# {\n#   'total': 100,\n#   'with_intro': 0,\n#   'with_article': 0,\n#   'unused_headlines': 50,\n#   'unused_intros': 0,\n#   'unused_articles': 0\n# }\n</code></pre>"},{"location":"library-usage/#custom-configuration","title":"Custom Configuration","text":"<p>Customize the provider behavior:</p> <pre><code>from faker import Faker\nfrom faker_news import NewsProvider, LLMClientConfig\n\n# Configure LLM settings\nllm_config = LLMClientConfig(\n    api_key=\"your-api-key\",\n    base_url=\"https://api.openai.com/v1\",\n    model_headlines=\"gpt-4o-mini\",  # Fast model for headlines\n    model_writing=\"gpt-4o\"           # Better model for full articles\n)\n\n# Configure provider with custom settings\nfake = Faker()\nprovider = NewsProvider(\n    fake,\n    llm_config=llm_config,\n    db_path=\"/custom/path/cache.sqlite3\",  # Custom cache location\n    min_headline_pool=50,                   # Keep 50 unused headlines\n    headline_batch=60,                      # Generate 60 at a time\n    intro_batch=30,                         # Generate 30 intros at a time\n    article_batch=15                        # Generate 15 articles at a time\n)\nfake.add_provider(provider)\n</code></pre>"},{"location":"library-usage/#error-handling","title":"Error Handling","text":"<p>Handle errors gracefully:</p> <pre><code>from faker import Faker\nfrom faker_news import NewsProvider\n\nfake = Faker()\nfake.add_provider(NewsProvider(fake))\n\ntry:\n    headline = fake.news_headline()\n    print(f\"Generated: {headline}\")\nexcept Exception as e:\n    print(f\"Error generating content: {e}\")\n    # Possible errors:\n    # - API key not configured\n    # - Network connection issues\n    # - LLM API errors\n    # - Rate limiting\n</code></pre>"},{"location":"library-usage/#advanced-examples","title":"Advanced Examples","text":""},{"location":"library-usage/#generate-multiple-complete-articles","title":"Generate Multiple Complete Articles","text":"<pre><code>def generate_news_articles(count=10):\n    \"\"\"Generate multiple complete news articles.\"\"\"\n    from faker import Faker\n    from faker_news import NewsProvider\n\n    fake = Faker()\n    fake.add_provider(NewsProvider(fake))\n\n    # Preload for performance\n    fake.news_preload_headlines(count)\n\n    articles = []\n    for _ in range(count):\n        headline = fake.news_headline()\n        intro = fake.news_intro(headline=headline)\n        article = fake.news_article(headline=headline, words=500)\n\n        articles.append({\n            'headline': headline,\n            'intro': intro,\n            'article': article\n        })\n\n    return articles\n\n# Use it\narticles = generate_news_articles(10)\nfor article in articles:\n    print(article['headline'])\n    print('-' * 70)\n    print(article['intro'])\n    print()\n    print(article['article'])\n    print('=' * 70)\n    print()\n</code></pre>"},{"location":"library-usage/#populate-database-for-testing","title":"Populate Database for Testing","text":"<pre><code>from faker import Faker\nfrom faker_news import NewsProvider\n\ndef populate_test_database():\n    \"\"\"Populate database with fake news for testing.\"\"\"\n    fake = Faker()\n    fake.add_provider(NewsProvider(fake))\n\n    # Generate 100 headlines\n    print(\"Generating headlines...\")\n    fake.news_preload_headlines(100)\n\n    # Generate intros and articles for 50 of them\n    print(\"Generating articles...\")\n    for i in range(50):\n        headline = fake.news_headline()\n        fake.news_intro(headline=headline)\n        fake.news_article(headline=headline, words=600)\n\n        if (i + 1) % 10 == 0:\n            print(f\"Generated {i + 1} complete articles...\")\n\n    # Show statistics\n    stats = fake.news_stats()\n    print(f\"\\nCache populated:\")\n    print(f\"  Total items: {stats['total']}\")\n    print(f\"  With intros: {stats['with_intro']}\")\n    print(f\"  With articles: {stats['with_article']}\")\n\n# Run it\npopulate_test_database()\n</code></pre>"},{"location":"library-usage/#use-with-custom-faker-providers","title":"Use with Custom Faker Providers","text":"<p>Combine with other Faker providers:</p> <pre><code>from faker import Faker\nfrom faker_news import NewsProvider\n\nfake = Faker()\nfake.add_provider(NewsProvider(fake))\n\n# Generate a fake news article with fake author and date\narticle_data = {\n    'headline': fake.news_headline(),\n    'author': fake.name(),\n    'date': fake.date_between(start_date='-30d', end_date='today'),\n    'category': fake.random_element(['Politics', 'Technology', 'Science', 'Sports']),\n    'intro': fake.news_intro(),\n    'article': fake.news_article(words=500)\n}\n\nprint(article_data)\n</code></pre>"},{"location":"library-usage/#next-steps","title":"Next Steps","text":"<ul> <li>CLI Reference - Use faker-news from the command line</li> <li>Cache Management - Advanced cache management</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"llm-providers/","title":"LLM Providers","text":"<p>faker-news works with any OpenAI-compatible LLM API. This guide provides detailed setup instructions for popular providers.</p>"},{"location":"llm-providers/#openai","title":"OpenAI","text":""},{"location":"llm-providers/#setup","title":"Setup","text":"Using Setup WizardUsing Environment VariablesUsing Python Config <pre><code>faker-news setup\n# Select \"OpenAI\" when prompted\n</code></pre> <pre><code>export OPENAI_API_KEY=\"sk-...\"\nexport OPENAI_BASE_URL=\"https://api.openai.com/v1\"  # Optional\n</code></pre> <pre><code>from faker_news import LLMClientConfig\n\nllm_config = LLMClientConfig(\n    api_key=\"sk-...\",\n    base_url=\"https://api.openai.com/v1\",\n    model_headlines=\"gpt-4o-mini\",\n    model_writing=\"gpt-4o\"\n)\n</code></pre>"},{"location":"llm-providers/#recommended-models","title":"Recommended Models","text":"Purpose Model Speed Quality Cost Headlines <code>gpt-4o-mini</code> \u26a1\u26a1\u26a1 \u2b50\u2b50\u2b50 \ud83d\udcb0 Headlines <code>gpt-3.5-turbo</code> \u26a1\u26a1\u26a1 \u2b50\u2b50 \ud83d\udcb0 Articles <code>gpt-4o-mini</code> \u26a1\u26a1\u26a1 \u2b50\u2b50\u2b50 \ud83d\udcb0 Articles (Quality) <code>gpt-4o</code> \u26a1\u26a1 \u2b50\u2b50\u2b50\u2b50\u2b50 \ud83d\udcb0\ud83d\udcb0\ud83d\udcb0 Articles (Quality) <code>gpt-4-turbo</code> \u26a1\u26a1 \u2b50\u2b50\u2b50\u2b50 \ud83d\udcb0\ud83d\udcb0\ud83d\udcb0"},{"location":"llm-providers/#example-configuration","title":"Example Configuration","text":"<pre><code>from faker import Faker\nfrom faker_news import NewsProvider, LLMClientConfig\n\n# Balanced setup (recommended)\nllm_config = LLMClientConfig(\n    model_headlines=\"gpt-4o-mini\",  # Fast and cheap\n    model_writing=\"gpt-4o-mini\"     # Good quality\n)\n\n# Quality-focused setup\nllm_config = LLMClientConfig(\n    model_headlines=\"gpt-4o-mini\",  # Headlines don't need premium\n    model_writing=\"gpt-4o\"          # Best quality for articles\n)\n\nfake = Faker()\nfake.add_provider(NewsProvider(fake, llm_config=llm_config))\n</code></pre>"},{"location":"llm-providers/#getting-an-api-key","title":"Getting an API Key","text":"<ol> <li>Sign up at platform.openai.com</li> <li>Navigate to API keys</li> <li>Create a new secret key</li> <li>Copy and store securely</li> </ol>"},{"location":"llm-providers/#alibaba-dashscope-qwen","title":"Alibaba DashScope (Qwen)","text":""},{"location":"llm-providers/#setup_1","title":"Setup","text":"Using Setup WizardUsing Environment VariablesUsing Python Config <pre><code>faker-news setup\n# Select \"Alibaba DashScope/Qwen\" when prompted\n</code></pre> <pre><code>export DASHSCOPE_API_KEY=\"sk-...\"\nexport DASHSCOPE_BASE_URL=\"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\"  # Optional\n</code></pre> <pre><code>from faker_news import LLMClientConfig\n\nllm_config = LLMClientConfig(\n    api_key=\"sk-...\",\n    base_url=\"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\",\n    model_headlines=\"qwen-flash\",\n    model_writing=\"qwen-plus\"\n)\n</code></pre>"},{"location":"llm-providers/#recommended-models_1","title":"Recommended Models","text":"Purpose Model Speed Quality Cost Headlines <code>qwen-flash</code> \u26a1\u26a1\u26a1 \u2b50\u2b50\u2b50 \ud83d\udcb0 Headlines <code>qwen-turbo</code> \u26a1\u26a1 \u2b50\u2b50\u2b50\u2b50 \ud83d\udcb0\ud83d\udcb0 Articles <code>qwen-flash</code> \u26a1\u26a1\u26a1 \u2b50\u2b50\u2b50 \ud83d\udcb0 Articles (Quality) <code>qwen-plus</code> \u26a1\u26a1 \u2b50\u2b50\u2b50\u2b50 \ud83d\udcb0\ud83d\udcb0 Articles (Quality) <code>qwen-max</code> \u26a1 \u2b50\u2b50\u2b50\u2b50\u2b50 \ud83d\udcb0\ud83d\udcb0\ud83d\udcb0"},{"location":"llm-providers/#example-configuration_1","title":"Example Configuration","text":"<pre><code>from faker import Faker\nfrom faker_news import NewsProvider, LLMClientConfig\n\n# Balanced setup\nllm_config = LLMClientConfig(\n    model_headlines=\"qwen-flash\",  # Fast and cheap\n    model_writing=\"qwen-plus\"      # Better quality\n)\n\nfake = Faker()\nfake.add_provider(NewsProvider(fake, llm_config=llm_config))\n</code></pre>"},{"location":"llm-providers/#getting-an-api-key_1","title":"Getting an API Key","text":"<ol> <li>Sign up at Alibaba Cloud Model Studio</li> <li>Enable DashScope API</li> <li>Create an API key</li> <li>Use international endpoint for global access</li> </ol>"},{"location":"llm-providers/#azure-openai","title":"Azure OpenAI","text":""},{"location":"llm-providers/#setup_2","title":"Setup","text":"<pre><code>from faker_news import LLMClientConfig\n\nllm_config = LLMClientConfig(\n    api_key=\"your-azure-api-key\",\n    base_url=\"https://{your-resource-name}.openai.azure.com/openai/deployments/{deployment-name}\",\n    model_headlines=\"gpt-4o-mini\",  # Your deployment name\n    model_writing=\"gpt-4\"           # Your deployment name\n)\n</code></pre>"},{"location":"llm-providers/#example-configuration_2","title":"Example Configuration","text":"<pre><code>from faker import Faker\nfrom faker_news import NewsProvider, LLMClientConfig\n\n# Azure OpenAI setup\nllm_config = LLMClientConfig(\n    api_key=\"abc123...\",\n    base_url=\"https://my-openai.openai.azure.com/openai/deployments/gpt-4o-mini\",\n    model_headlines=\"gpt-4o-mini\",\n    model_writing=\"gpt-4\"\n)\n\nfake = Faker()\nfake.add_provider(NewsProvider(fake, llm_config=llm_config))\n</code></pre> <p>Deployment Names</p> <p>Use your Azure deployment names as the model names. These may differ from OpenAI's model names.</p>"},{"location":"llm-providers/#getting-started-with-azure-openai","title":"Getting Started with Azure OpenAI","text":"<ol> <li>Create an Azure account</li> <li>Create an Azure OpenAI resource</li> <li>Deploy models (e.g., gpt-4o-mini, gpt-4)</li> <li>Get API key and endpoint from Azure portal</li> </ol>"},{"location":"llm-providers/#other-openai-compatible-apis","title":"Other OpenAI-Compatible APIs","text":"<p>faker-news works with any API that implements the OpenAI chat completions interface.</p>"},{"location":"llm-providers/#generic-setup","title":"Generic Setup","text":"<pre><code>from faker_news import LLMClientConfig\n\nllm_config = LLMClientConfig(\n    api_key=\"your-api-key\",\n    base_url=\"https://your-api-endpoint.com/v1\",\n    model_headlines=\"your-fast-model\",\n    model_writing=\"your-quality-model\"\n)\n</code></pre>"},{"location":"llm-providers/#compatible-providers","title":"Compatible Providers","text":"<p>These providers have OpenAI-compatible APIs:</p> <ul> <li>Anthropic (via compatibility layer)</li> <li>Groq - Ultra-fast inference</li> <li>Together.ai - Multiple open models</li> <li>Perplexity - Search-augmented models</li> <li>Anyscale - Llama and Mistral models</li> <li>DeepSeek - Chinese LLM provider</li> <li>Local models (via LM Studio, Ollama, vLLM, etc.)</li> </ul>"},{"location":"llm-providers/#example-local-llm-with-ollama","title":"Example: Local LLM with Ollama","text":"<pre><code>from faker_news import LLMClientConfig\n\n# Run Ollama locally (ollama serve)\nllm_config = LLMClientConfig(\n    api_key=\"not-needed\",  # Ollama doesn't require API key\n    base_url=\"http://localhost:11434/v1\",\n    model_headlines=\"llama3.1\",\n    model_writing=\"llama3.1\"\n)\n</code></pre>"},{"location":"llm-providers/#example-groq","title":"Example: Groq","text":"<pre><code>from faker_news import LLMClientConfig\n\nllm_config = LLMClientConfig(\n    api_key=\"gsk_...\",\n    base_url=\"https://api.groq.com/openai/v1\",\n    model_headlines=\"llama-3.1-70b-versatile\",\n    model_writing=\"llama-3.1-70b-versatile\"\n)\n</code></pre>"},{"location":"llm-providers/#choosing-a-provider","title":"Choosing a Provider","text":""},{"location":"llm-providers/#by-use-case","title":"By Use Case","text":"Use Case Recommended Provider Why Production OpenAI Best reliability and quality Development OpenAI (gpt-4o-mini) Good balance of cost/quality High-volume Alibaba Qwen Cost-effective for bulk Local/Offline Ollama + local models No API costs, privacy Fast inference Groq Ultra-fast responses Budget-conscious Alibaba Qwen Lowest cost per token"},{"location":"llm-providers/#by-region","title":"By Region","text":"Region Recommended Provider North America OpenAI, Azure OpenAI Europe OpenAI, Azure OpenAI Asia-Pacific Alibaba DashScope China Alibaba DashScope Global OpenAI"},{"location":"llm-providers/#by-requirements","title":"By Requirements","text":"<p>Need best quality? \u2192 OpenAI GPT-4o</p> <p>Need lowest cost? \u2192 Alibaba Qwen Flash</p> <p>Need fastest speed? \u2192 Groq with Llama</p> <p>Need privacy? \u2192 Local models (Ollama)</p> <p>Need reliability? \u2192 OpenAI or Azure OpenAI</p>"},{"location":"llm-providers/#cost-optimization","title":"Cost Optimization","text":""},{"location":"llm-providers/#model-selection-strategy","title":"Model Selection Strategy","text":"<pre><code>from faker_news import LLMClientConfig\n\n# Cost-optimized: use cheapest models\nllm_config = LLMClientConfig(\n    model_headlines=\"gpt-4o-mini\",  # Cheap for simple headlines\n    model_writing=\"gpt-4o-mini\"     # Still good quality\n)\n\n# Quality-optimized: use better model for articles only\nllm_config = LLMClientConfig(\n    model_headlines=\"gpt-4o-mini\",  # Cheap for headlines\n    model_writing=\"gpt-4o\"          # Premium for articles\n)\n</code></pre>"},{"location":"llm-providers/#batch-size-tuning","title":"Batch Size Tuning","text":"<p>Larger batches = fewer API calls = lower cost:</p> <pre><code>from faker_news import NewsProvider\n\n# Cost-optimized setup\nprovider = NewsProvider(\n    fake,\n    llm_config=llm_config,\n    headline_batch=200,  # Large batches\n    article_batch=50\n)\n</code></pre>"},{"location":"llm-providers/#caching-strategy","title":"Caching Strategy","text":"<p>Maximize cache reuse to minimize API calls:</p> <pre><code>from faker import Faker\nfrom faker_news import NewsProvider\n\nfake = Faker()\nfake.add_provider(NewsProvider(fake))\n\n# Preload once, use many times\nfake.news_preload_headlines(1000)\n\n# Reuse content\nheadlines = [fake.news_headline() for _ in range(500)]\nfake.news_reset(\"reuse\")\nheadlines = [fake.news_headline() for _ in range(500)]  # No new API calls\n</code></pre>"},{"location":"llm-providers/#troubleshooting","title":"Troubleshooting","text":""},{"location":"llm-providers/#api-key-not-working","title":"API Key Not Working","text":"<ol> <li>Verify key format matches provider</li> <li>Check environment variables: <code>echo $OPENAI_API_KEY</code></li> <li>Test with curl:    <pre><code>curl https://api.openai.com/v1/models \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"\n</code></pre></li> </ol>"},{"location":"llm-providers/#connection-errors","title":"Connection Errors","text":"<ol> <li>Verify base URL is correct</li> <li>Check network connectivity</li> <li>Verify API endpoint is accessible</li> <li>Check for firewall/proxy issues</li> </ol>"},{"location":"llm-providers/#rate-limiting","title":"Rate Limiting","text":"<p>If you hit rate limits:</p> <ol> <li>Reduce batch sizes</li> <li>Add delays between generations</li> <li>Upgrade to higher tier</li> <li>Switch to provider with higher limits</li> </ol>"},{"location":"llm-providers/#model-not-found","title":"Model Not Found","text":"<p>If you get \"model not found\" errors:</p> <ol> <li>Verify model name is correct</li> <li>Check if you have access to that model</li> <li>For Azure: use deployment name, not model name</li> </ol>"},{"location":"llm-providers/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration - Customize batch sizes and settings</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"quick-start/","title":"Quick Start","text":"<p>Get started with faker-news in minutes! This guide assumes you've already installed the package and configured your API key.</p>"},{"location":"quick-start/#using-as-a-faker-provider","title":"Using as a Faker Provider","text":"<p>The primary way to use faker-news is as a Faker provider:</p> <pre><code>from faker import Faker\nfrom faker_news import NewsProvider\n\n# Create Faker instance and add provider\nfake = Faker()\nfake.add_provider(NewsProvider(fake))\n\n# Generate a headline\nheadline = fake.news_headline()\nprint(headline)\n# Output: \"Tech Giant Announces Revolutionary AI Breakthrough\"\n\n# Generate an intro for that headline\nintro = fake.news_intro(headline=headline)\nprint(intro)\n# Output: \"In a surprising move today, the company unveiled...\"\n\n# Generate a full article\narticle = fake.news_article(headline=headline, words=500)\nprint(article)\n</code></pre>"},{"location":"quick-start/#using-the-cli","title":"Using the CLI","text":"<p>Prefer command-line tools? Use the faker-news CLI:</p> <pre><code># Generate a headline\nfaker-news headline\n\n# Generate an intro (with auto-generated headline)\nfaker-news intro\n\n# Generate a full article\nfaker-news article\n\n# Generate a longer article\nfaker-news article --words 800\n\n# Generate content for a specific headline\nfaker-news intro --headline \"Breaking: New Discovery Announced\"\nfaker-news article --headline \"Breaking: New Discovery Announced\"\n\n# Always generate fresh content (skip cache)\nfaker-news headline --new\nfaker-news article --new --words 500\n</code></pre>"},{"location":"quick-start/#understanding-consumption","title":"Understanding Consumption","text":"<p>By default, items behave differently in the library vs CLI:</p> <p>Library (Python): Items are marked as \"used\" after fetching</p> <pre><code># These mark items as used\nheadline1 = fake.news_headline()\nheadline2 = fake.news_headline()  # Different headline\n\n# Fetch without marking as used\nheadline3 = fake.news_headline(consume=False)\nheadline4 = fake.news_headline(consume=False)  # Could be same as headline3\n</code></pre> <p>CLI: Items are NOT marked as used (for easier testing)</p> <pre><code># These don't mark as used - you can repeat the command\nfaker-news headline\nfaker-news headline  # Might show the same headline\n\n# Mark as used with --consume flag\nfaker-news headline --consume\nfaker-news headline --consume  # Different headline\n</code></pre>"},{"location":"quick-start/#preloading-content","title":"Preloading Content","text":"<p>For better performance, preload headlines in bulk:</p> <pre><code># Preload 100 headlines\nfake.news_preload_headlines(100)\n\n# Now fetching is instant (no API calls)\nheadlines = [fake.news_headline() for _ in range(50)]\n</code></pre> <p>Or via CLI:</p> <pre><code># Preload 50 headlines\nfaker-news preload --n 50\n\n# Preload with full content (intros + articles)\nfaker-news preload --n 50 --with-intros --with-articles\n\n# Smart populate: ensure 50 unused headlines exist with full content\n# (reuses existing, generates new only if needed)\nfaker-news preload --n 50 --populate\n</code></pre>"},{"location":"quick-start/#checking-cache-status","title":"Checking Cache Status","text":"<p>See what's in your cache:</p> <pre><code>stats = fake.news_stats()\nprint(stats)\n# {\n#   'total': 100,\n#   'with_intro': 50,\n#   'with_article': 30,\n#   'unused_headlines': 75,\n#   'unused_intros': 40,\n#   'unused_articles': 25\n# }\n</code></pre> <p>Or via CLI:</p> <pre><code>faker-news stats\n</code></pre>"},{"location":"quick-start/#complete-example","title":"Complete Example","text":"<p>Here's a complete example generating a full news article:</p> <pre><code>from faker import Faker\nfrom faker_news import NewsProvider\n\nfake = Faker()\nfake.add_provider(NewsProvider(fake))\n\n# Preload for performance\nfake.news_preload_headlines(50)\n\n# Generate a complete article\nheadline = fake.news_headline()\nintro = fake.news_intro(headline=headline)\narticle = fake.news_article(headline=headline, words=600)\n\n# Print formatted output\nprint(\"=\" * 70)\nprint(headline)\nprint(\"=\" * 70)\nprint()\nprint(intro)\nprint()\nprint(article)\nprint()\nprint(\"=\" * 70)\n</code></pre>"},{"location":"quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Library Usage - Detailed guide on using the Python API</li> <li>CLI Reference - Complete CLI command reference</li> <li>Cache Management - Advanced cache management techniques</li> <li>Configuration - Customize batch sizes, models, and more</li> </ul>"},{"location":"setup/","title":"Setup &amp; Configuration","text":"<p>This guide will walk you through setting up your LLM API credentials and configuring faker-news.</p>"},{"location":"setup/#interactive-setup-recommended","title":"Interactive Setup (Recommended)","text":"<p>The easiest way to configure faker-news is using the interactive setup wizard:</p> <pre><code>faker-news setup\n</code></pre> <p>This wizard will:</p> <ol> <li>\u2705 Check if you already have API keys configured (in keyring or environment)</li> <li>\ud83d\udd11 Guide you through selecting and configuring an LLM provider</li> <li>\ud83d\udcbe Securely store your API key in your system's credential manager</li> <li>\ud83e\uddea Test the connection with a sample generation</li> <li>\u2728 Confirm everything is working</li> </ol>"},{"location":"setup/#secure-api-key-storage","title":"Secure API Key Storage","text":"<p>faker-news uses Python's <code>keyring</code> library to store API keys securely in your system's native credential manager:</p> Platform Storage Location macOS Keychain Windows Credential Manager Linux Secret Service (GNOME Keyring/KWallet) <p>This is more secure than environment variables or config files, as the credentials are encrypted and managed by your OS.</p>"},{"location":"setup/#manual-configuration","title":"Manual Configuration","text":"<p>If you prefer to configure manually, you have three options:</p>"},{"location":"setup/#option-1-system-keyring-secure","title":"Option 1: System Keyring (Secure)","text":"<p>Set your API key in the system keyring using Python:</p> <pre><code>import keyring\n\n# For OpenAI\nkeyring.set_password(\"faker-news\", \"openai\", \"sk-your-api-key-here\")\n\n# For Alibaba DashScope/Qwen\nkeyring.set_password(\"faker-news\", \"dashscope\", \"sk-your-api-key-here\")\n</code></pre>"},{"location":"setup/#option-2-environment-variables","title":"Option 2: Environment Variables","text":"<p>Set environment variables (less secure, but useful for CI/CD):</p> OpenAIAlibaba DashScope/QwenAzure OpenAI <pre><code>export OPENAI_API_KEY=\"sk-your-api-key-here\"\nexport OPENAI_BASE_URL=\"https://api.openai.com/v1\"  # Optional\n</code></pre> <pre><code>export DASHSCOPE_API_KEY=\"sk-your-api-key-here\"\nexport DASHSCOPE_BASE_URL=\"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\"  # Optional\n</code></pre> <pre><code>export OPENAI_API_KEY=\"your-azure-api-key\"\nexport OPENAI_BASE_URL=\"https://your-resource.openai.azure.com/openai/deployments/your-deployment\"\n</code></pre>"},{"location":"setup/#option-3-programmatic-configuration","title":"Option 3: Programmatic Configuration","text":"<p>Pass credentials directly in your code:</p> <pre><code>from faker import Faker\nfrom faker_news import NewsProvider, LLMClientConfig\n\n# Configure LLM client\nllm_config = LLMClientConfig(\n    api_key=\"sk-your-api-key-here\",\n    base_url=\"https://api.openai.com/v1\",  # Optional\n    model_headlines=\"gpt-4o-mini\",         # Optional\n    model_writing=\"gpt-4o-mini\"            # Optional\n)\n\n# Create provider with config\nfake = Faker()\nprovider = NewsProvider(fake, llm_config=llm_config)\nfake.add_provider(provider)\n</code></pre>"},{"location":"setup/#api-key-lookup-order","title":"API Key Lookup Order","text":"<p>faker-news searches for API keys in this order:</p> <ol> <li>System keyring (checked first via <code>keyring.get_password()</code>)</li> <li>Environment variables (<code>OPENAI_API_KEY</code> or <code>DASHSCOPE_API_KEY</code>)</li> <li>Explicit config (passed to <code>LLMClientConfig</code>)</li> </ol> <p>This allows you to use the secure keyring for local development while using environment variables in CI/CD pipelines.</p>"},{"location":"setup/#testing-your-configuration","title":"Testing Your Configuration","text":"<p>After configuring your API key, test it:</p>"},{"location":"setup/#using-the-cli","title":"Using the CLI","text":"<pre><code># The setup wizard includes a test\nfaker-news setup\n\n# Or manually test by generating a headline\nfaker-news headline\n</code></pre>"},{"location":"setup/#using-python","title":"Using Python","text":"<pre><code>from faker import Faker\nfrom faker_news import NewsProvider\n\nfake = Faker()\nfake.add_provider(NewsProvider(fake))\n\n# This will fail if credentials are not configured\ntry:\n    headline = fake.news_headline()\n    print(f\"\u2705 Success! Generated: {headline}\")\nexcept Exception as e:\n    print(f\"\u274c Error: {e}\")\n</code></pre>"},{"location":"setup/#database-location","title":"Database Location","text":"<p>By default, faker-news stores the cache in platform-specific directories:</p> Platform Cache Location Linux <code>~/.cache/faker-news/cache.sqlite3</code> macOS <code>~/Library/Caches/faker-news/cache.sqlite3</code> Windows <code>%LOCALAPPDATA%\\faker-news\\cache\\cache.sqlite3</code>"},{"location":"setup/#using-a-custom-database-location","title":"Using a Custom Database Location","text":"<p>You can override the default location:</p> <p>Via Python:</p> <pre><code>provider = NewsProvider(fake, db_path=\"/custom/path/cache.sqlite3\")\n</code></pre> <p>Via CLI:</p> <pre><code>faker-news headline --db /custom/path/cache.sqlite3\n</code></pre>"},{"location":"setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"setup/#api-key-not-found","title":"API Key Not Found","text":"<p>If you get \"API key not configured\" errors:</p> <ol> <li>Run <code>faker-news setup</code> to check your configuration</li> <li>Verify environment variables: <code>echo $OPENAI_API_KEY</code></li> <li>Check keyring: Run the Python snippet above to view stored keys</li> </ol>"},{"location":"setup/#connection-errors","title":"Connection Errors","text":"<p>If you get connection errors:</p> <ol> <li>Verify your API key is correct</li> <li>Check your internet connection</li> <li>Verify the <code>base_url</code> is correct for your provider</li> <li>Check if your LLM provider is experiencing downtime</li> </ol>"},{"location":"setup/#keyring-issues-on-linux","title":"Keyring Issues on Linux","text":"<p>If keyring doesn't work on Linux, you may need to install a secret service backend:</p> <pre><code># Ubuntu/Debian\nsudo apt-get install gnome-keyring\n\n# Fedora\nsudo dnf install gnome-keyring\n</code></pre> <p>Or use environment variables as a fallback.</p>"},{"location":"setup/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Start generating content</li> <li>LLM Providers - Detailed guides for specific LLM providers</li> <li>Configuration - Customize models, batch sizes, and more</li> </ul>"}]}